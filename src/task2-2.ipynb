{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45413b9dde2d0031",
   "metadata": {},
   "source": [
    "本方案参考F-VLM设计\n",
    "\n",
    "需要微调"
   ]
  },
  {
   "cell_type": "code",
   "id": "6a8af5b19cd3784f",
   "metadata": {},
   "source": [
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from pycocotools import mask as mask_utils\n",
    "import json\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "    \n",
    "# 使用 autocast 自动将计算转换为半精度\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionTransformer\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from detectron2.modeling.meta_arch.rcnn import GeneralizedRCNN\n",
    "from detectron2.modeling.proposal_generator.rpn import RPN\n",
    "from detectron2.modeling.proposal_generator.proposal_utils import add_ground_truth_to_proposals\n",
    "from detectron2.modeling.roi_heads.roi_heads import StandardROIHeads\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, fast_rcnn_inference\n",
    "from detectron2.modeling.roi_heads.mask_head import MaskRCNNConvUpsampleHead\n",
    "from detectron2.modeling.box_regression import _dense_box_regression_loss\n",
    "from detectron2.modeling.poolers import convert_boxes_to_pooler_format\n",
    "from detectron2.modeling.postprocessing import detector_postprocess\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.structures import Boxes, Instances, BitMasks\n",
    "from detectron2.structures.boxes import pairwise_iou\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.layers import ROIAlign\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.utils.visualizer import Visualizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f54109a1cced2b37",
   "metadata": {},
   "source": [
    "val_json = \"../data/COCO/annotations/instances_val2017.json\"\n",
    "filtered_val_json = \"../data/COCO/annotations/instances_val2017_filtered.json\"\n",
    "val_images = \"../data/COCO/val2017\"\n",
    "\n",
    "train_json = \"../data/COCO/annotations/instances_train2017.json\"\n",
    "filtered_train_json = \"../data/COCO/annotations/instances_train2017_filtered.json\"\n",
    "train_images = \"../data/COCO/train2017\"\n",
    "\n",
    "val_small_json = \"./val_small.json\"\n",
    "\n",
    "MRCNN_PATH = \"../model/model_final_f10217.pkl\"\n",
    "CONFIG_FILE = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "\n",
    "CLIP_PATH = \"../model/clip-vit-patch32/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268\"\n",
    "MyMRCNN_PATH = \"../model/my_mask_rcnn.pkl\"\n",
    "MyMRCNN_CLSFREE_PATH = \"../model/my_clsfree_mask_rcnn.pkl\"\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE))\n",
    "cfg.MODEL.WEIGHTS = MRCNN_PATH\n",
    "cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG = True\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "\n",
    "deterministic = False\n",
    "half = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cbc9eeb1f59346ee",
   "metadata": {},
   "source": [
    "把训练集中标注为空的清理掉"
   ]
  },
  {
   "cell_type": "code",
   "id": "b46dfb4415c72be0",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "with open(train_json, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "valid_image_ids = set(ann['image_id'] for ann in data['annotations'] if ann)\n",
    "\n",
    "filtered_images = [img for img in data['images'] if img['id'] in valid_image_ids]\n",
    "\n",
    "filtered_annotations = [ann for ann in data['annotations'] if ann and ann['image_id'] in valid_image_ids]\n",
    "\n",
    "categories = data.get('categories', [])\n",
    "\n",
    "# 生成新的 COCO json\n",
    "new_data = {\n",
    "    'images': filtered_images,\n",
    "    'annotations': filtered_annotations,\n",
    "    'categories': categories\n",
    "}\n",
    "\n",
    "# 保存新 json\n",
    "with open(filtered_train_json, 'w') as f:\n",
    "    json.dump(new_data, f, indent=4)\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e100cef10da73a0",
   "metadata": {},
   "source": [
    "实现余弦退火调度器"
   ]
  },
  {
   "cell_type": "code",
   "id": "110daadabaf1d85f",
   "metadata": {},
   "source": [
    "def cosine_scheduler_with_warmup(optimizer, warmup_steps, total_steps, min_lr_ratio=0.1):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        progress = (current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return min_lr_ratio + (1 - min_lr_ratio) * cosine_decay\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f546be0ce9296283",
   "metadata": {},
   "source": [
    "这里有一部分元件需要从头训练，因此给出两种初始化代码备用"
   ]
  },
  {
   "cell_type": "code",
   "id": "e5fb97405543867e",
   "metadata": {},
   "source": [
    "def init_weights_kaiming(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "def init_weights_xavier(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4508c8322ce0a17",
   "metadata": {},
   "source": [
    "这里还存在对比损失，这里也给出 InfoNCE 对比损失的实现"
   ]
  },
  {
   "cell_type": "code",
   "id": "d94b9d8e19f3d680",
   "metadata": {},
   "source": [
    "def contrastive_loss(scores, gt_classes, temperature=0.07):\n",
    "    # softmax 温度缩放\n",
    "    logits = scores / temperature\n",
    "\n",
    "    # 使用交叉熵作为对比损失\n",
    "    with autocast(enabled=False):\n",
    "        loss = F.cross_entropy(logits.float(), gt_classes.long())\n",
    "    return loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e98567a453a8bcee",
   "metadata": {},
   "source": [
    "按照 F-VLM 的设计，CLIP可以分为两部分，分别是特征提取与最后的Pooling层。\n",
    "\n",
    "这里将其分出来"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "class CLIPVisionTransformerSplit(CLIPVisionTransformer):\n",
    "    def forward_features(self, pixel_values):\n",
    "        \"\"\"对应 feature extractor 部分\"\"\"\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "        hidden_states = self.pre_layrnorm(hidden_states)\n",
    "        encoder_outputs = self.encoder(inputs_embeds=hidden_states)\n",
    "        return encoder_outputs[0]  # last_hidden_state\n",
    "\n",
    "    def forward_pool(self, last_hidden_state):\n",
    "        \"\"\"对应 last feature pooling layer 部分，这一部分的输出不能直接用，还需要 visual projection 来投影\"\"\"\n",
    "        pooled_output = self.post_layernorm(last_hidden_state)\n",
    "        return pooled_output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14d6ae44a95fe26b",
   "metadata": {},
   "source": [
    "class TwoStageCLIPModel(CLIPModel):\n",
    "    \"\"\"\n",
    "    拓展版 CLIP 模型，支持显式分离视觉编码的两个阶段：\n",
    "    1. Feature Extractor (patch embedding + transformer encoder)\n",
    "    2. Last Feature Pooling Layer (CLS pooling + LayerNorm)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.vision_model = CLIPVisionTransformerSplit(config.vision_config)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        model = super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
    "\n",
    "        vision_config = model.config.vision_config\n",
    "        new_vision_model = CLIPVisionTransformerSplit(vision_config)\n",
    "        new_vision_model.load_state_dict(model.vision_model.state_dict())\n",
    "        model.vision_model = new_vision_model\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_image_features_stage1(self, pixel_values: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        获取图像的 patch-level 特征 (Feature Extractor 输出)\n",
    "        对应 self.vision_model.embeddings + self.vision_model.encoder\n",
    "        这里的输出中，seq_len维度的首位是CLS，其余是图像各分块的embeddings。\n",
    "        \"\"\"\n",
    "        return self.vision_model.forward_features(pixel_values)  # shape: (batch, seq_len, hidden_dim)\n",
    "\n",
    "    def get_image_features_stage2(self, last_hidden_state: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        从 stage1 输出计算 pooled image feature\n",
    "        对应 self.vision_model.post_layernorm(CLS token) + self.visual_projection(pooled_state)\n",
    "        \"\"\"\n",
    "        pooled_state = self.vision_model.forward_pool(last_hidden_state)  # shape: (batch, hidden_dim)\n",
    "        return self.visual_projection(pooled_state)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "507f3c3345e5c18e",
   "metadata": {},
   "source": "原本的 RPN 设计的 forward 需要传入的东西拿不出来，这里新建子类重写，另外移除事件存储部分"
  },
  {
   "cell_type": "code",
   "id": "e38b7b63dc4140e2",
   "metadata": {},
   "source": [
    "class MyRPN(RPN):\n",
    "    def forward(self, image_sizes, features, gt_instances=None):\n",
    "        \"\"\"\n",
    "        原本传入 image 只为了 image_size，这次直接传入image_size，避免还要image\n",
    "        \"\"\"\n",
    "        features = [features[f] for f in self.in_features]\n",
    "        anchors = self.anchor_generator(features)\n",
    "\n",
    "        pred_objectness_logits, pred_anchor_deltas = self.rpn_head(features)\n",
    "        # Transpose the Hi*Wi*A dimension to the middle:\n",
    "        pred_objectness_logits = [\n",
    "            # (N, A, Hi, Wi) -> (N, Hi, Wi, A) -> (N, Hi*Wi*A)\n",
    "            score.permute(0, 2, 3, 1).flatten(1)\n",
    "            for score in pred_objectness_logits\n",
    "        ]\n",
    "        pred_anchor_deltas = [\n",
    "            # (N, A*B, Hi, Wi) -> (N, A, B, Hi, Wi) -> (N, Hi, Wi, A, B) -> (N, Hi*Wi*A, B)\n",
    "            x.view(x.shape[0], -1, self.anchor_generator.box_dim, x.shape[-2], x.shape[-1])\n",
    "            .permute(0, 3, 4, 1, 2)\n",
    "            .flatten(1, -2)\n",
    "            for x in pred_anchor_deltas\n",
    "        ]\n",
    "\n",
    "        if self.training:\n",
    "            assert gt_instances is not None, \"RPN requires gt_instances in training!\"\n",
    "            gt_labels, gt_boxes = self.label_and_sample_anchors(anchors, gt_instances)\n",
    "            losses = self.losses(\n",
    "                anchors, pred_objectness_logits, gt_labels, pred_anchor_deltas, gt_boxes\n",
    "            )\n",
    "        else:\n",
    "            losses = {}\n",
    "        proposals = self.predict_proposals(\n",
    "            anchors, pred_objectness_logits, pred_anchor_deltas, image_sizes\n",
    "        )\n",
    "        return proposals, losses\n",
    "    \n",
    "    @torch.jit.unused\n",
    "    def losses(self, anchors, pred_objectness_logits, gt_labels, pred_anchor_deltas, gt_boxes):\n",
    "        num_images = len(gt_labels)\n",
    "        gt_labels = torch.stack(gt_labels)\n",
    "        pos_mask = gt_labels == 1\n",
    "\n",
    "        localization_loss = _dense_box_regression_loss(\n",
    "            anchors,\n",
    "            self.box2box_transform,\n",
    "            pred_anchor_deltas,\n",
    "            gt_boxes,\n",
    "            pos_mask,\n",
    "            box_reg_loss_type=self.box_reg_loss_type,\n",
    "            smooth_l1_beta=self.smooth_l1_beta,\n",
    "        )\n",
    "\n",
    "        valid_mask = gt_labels >= 0\n",
    "        objectness_loss = F.binary_cross_entropy_with_logits(\n",
    "            torch.cat(pred_objectness_logits, dim=1)[valid_mask],\n",
    "            gt_labels[valid_mask].to(torch.float32),\n",
    "            reduction=\"sum\",\n",
    "        )\n",
    "        normalizer = self.batch_size_per_image * num_images\n",
    "        losses = {\n",
    "            \"loss_rpn_cls\": objectness_loss / normalizer,\n",
    "            \"loss_rpn_loc\": localization_loss / normalizer,\n",
    "        }\n",
    "        losses = {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n",
    "        return losses"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "228093038caeb3b7",
   "metadata": {},
   "source": [
    "Box Predictor需要重写 forward 与 loss\n",
    "\n",
    "loss 的类别应该改为对比损失"
   ]
  },
  {
   "cell_type": "code",
   "id": "663d7a67e9bca82e",
   "metadata": {},
   "source": [
    "class MyBoxPredictor(FastRCNNOutputLayers):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: per-region features of shape (N, ...) for N bounding boxes to predict.\n",
    "\n",
    "        Returns:\n",
    "            (Tensor, Tensor):\n",
    "            First tensor: shape (N,K+1), scores for each of the N box. Each row contains the\n",
    "            scores for K object categories and 1 background class.\n",
    "\n",
    "            Second tensor: bounding box regression deltas for each box. Shape is shape (N,Kx4),\n",
    "            or (N,4) for class-agnostic regression.\n",
    "        \"\"\"\n",
    "        if x.dim() > 2:\n",
    "            x = torch.flatten(x, start_dim=1)\n",
    "        # scores = self.cls_score(x)  # scores不需要有\n",
    "        proposal_deltas = self.bbox_pred(x)\n",
    "        return None, proposal_deltas\n",
    "    \n",
    "    def losses(self, predictions, proposals):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were used\n",
    "                to compute predictions. The fields ``proposal_boxes``, ``gt_boxes``,\n",
    "                ``gt_classes`` are expected.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Tensor]: dict of losses\n",
    "        \"\"\"\n",
    "        scores, proposal_deltas = predictions\n",
    "\n",
    "        # parse classification outputs\n",
    "        gt_classes = torch.cat([p.gt_classes for p in proposals], dim=0)\n",
    "\n",
    "        # parse box regression outputs\n",
    "        proposal_boxes = torch.cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n",
    "        assert not proposal_boxes.requires_grad, \"Proposals should not require gradients!\"\n",
    "        # If \"gt_boxes\" does not exist, the proposals must be all negative and\n",
    "        # should not be included in regression loss computation.\n",
    "        # Here we just use proposal_boxes as an arbitrary placeholder because its\n",
    "        # value won't be used in self.box_reg_loss().\n",
    "        gt_boxes = torch.cat(\n",
    "            [(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        losses = {\n",
    "            \"loss_cls\": contrastive_loss(scores, gt_classes),\n",
    "            \"loss_box_reg\": self.box_reg_loss(\n",
    "                proposal_boxes, gt_boxes, proposal_deltas, gt_classes\n",
    "            ),\n",
    "        }\n",
    "        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f8a2c80246e7d60",
   "metadata": {},
   "source": [
    "重写Mask Head"
   ]
  },
  {
   "cell_type": "code",
   "id": "57e5e41eaf5fb972",
   "metadata": {},
   "source": [
    "class MyMaskHead(MaskRCNNConvUpsampleHead):\n",
    "    def forward(self, x, instances):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input region feature(s) provided by :class:`ROIHeads`.\n",
    "            instances (list[Instances]): contains the boxes & labels corresponding\n",
    "                to the input features.\n",
    "                Exact format is up to its caller to decide.\n",
    "                Typically, this is the foreground instances in training, with\n",
    "                \"proposal_boxes\" field and other gt annotations.\n",
    "                In inference, it contains boxes that are already predicted.\n",
    "\n",
    "        Returns:\n",
    "            A dict of losses in training. The predicted \"instances\" in inference.\n",
    "        \"\"\"\n",
    "        x = self.layers(x)\n",
    "        if self.training:\n",
    "            return {\"loss_mask\": mask_rcnn_loss(x, instances, self.vis_period) * self.loss_weight}\n",
    "        else:\n",
    "            mask_rcnn_inference(x, instances)\n",
    "            return instances\n",
    "        \n",
    "@torch.jit.unused\n",
    "def mask_rcnn_loss(pred_mask_logits: torch.Tensor, instances, vis_period: int = 0):\n",
    "    \"\"\"\n",
    "    Compute the mask prediction loss defined in the Mask R-CNN paper.\n",
    "\n",
    "    Args:\n",
    "        pred_mask_logits (Tensor): A tensor of shape (B, C, Hmask, Wmask) or (B, 1, Hmask, Wmask)\n",
    "            for class-specific or class-agnostic, where B is the total number of predicted masks\n",
    "            in all images, C is the number of foreground classes, and Hmask, Wmask are the height\n",
    "            and width of the mask predictions. The values are logits.\n",
    "        instances (list[Instances]): A list of N Instances, where N is the number of images\n",
    "            in the batch. These instances are in 1:1\n",
    "            correspondence with the pred_mask_logits. The ground-truth labels (class, box, mask,\n",
    "            ...) associated with each instance are stored in fields.\n",
    "        vis_period (int): the period (in steps) to dump visualization.\n",
    "\n",
    "    Returns:\n",
    "        mask_loss (Tensor): A scalar tensor containing the loss.\n",
    "    \"\"\"\n",
    "    cls_agnostic_mask = pred_mask_logits.size(1) == 1\n",
    "    total_num_masks = pred_mask_logits.size(0)\n",
    "    mask_side_len = pred_mask_logits.size(2)\n",
    "    assert pred_mask_logits.size(2) == pred_mask_logits.size(3), \"Mask prediction must be square!\"\n",
    "\n",
    "    gt_classes = []\n",
    "    gt_masks = []\n",
    "    for instances_per_image in instances:\n",
    "        if len(instances_per_image) == 0:\n",
    "            continue\n",
    "        if not cls_agnostic_mask:\n",
    "            gt_classes_per_image = instances_per_image.gt_classes.to(dtype=torch.int64)\n",
    "            gt_classes.append(gt_classes_per_image)\n",
    "\n",
    "        gt_masks_per_image = instances_per_image.gt_masks.crop_and_resize(\n",
    "            instances_per_image.proposal_boxes.tensor, mask_side_len\n",
    "        ).to(device=pred_mask_logits.device)\n",
    "        # A tensor of shape (N, M, M), N=#instances in the image; M=mask_side_len\n",
    "        gt_masks.append(gt_masks_per_image)\n",
    "\n",
    "    if len(gt_masks) == 0:\n",
    "        return pred_mask_logits.sum() * 0\n",
    "\n",
    "    gt_masks = torch.cat(gt_masks, dim=0)\n",
    "\n",
    "    if cls_agnostic_mask:\n",
    "        pred_mask_logits = pred_mask_logits[:, 0]\n",
    "    else:\n",
    "        indices = torch.arange(total_num_masks)\n",
    "        gt_classes = torch.cat(gt_classes, dim=0)\n",
    "        pred_mask_logits = pred_mask_logits[indices, gt_classes]\n",
    "\n",
    "    gt_masks = gt_masks.to(dtype=torch.float32)\n",
    "\n",
    "    mask_loss = F.binary_cross_entropy_with_logits(pred_mask_logits, gt_masks, reduction=\"mean\")\n",
    "    return mask_loss\n",
    "\n",
    "def mask_rcnn_inference(pred_mask_logits: torch.Tensor, pred_instances):\n",
    "    cls_agnostic_mask = pred_mask_logits.size(1) == 1\n",
    "\n",
    "    if cls_agnostic_mask:\n",
    "        mask_probs_pred = pred_mask_logits.sigmoid()\n",
    "    else:\n",
    "        num_masks = pred_mask_logits.shape[0]\n",
    "        class_pred = torch.cat([i.pred_classes for i in pred_instances])\n",
    "        indices = torch.arange(num_masks, device=class_pred.device)\n",
    "        mask_probs_pred = pred_mask_logits[indices, class_pred][:, None].sigmoid()\n",
    "\n",
    "    num_boxes_per_image = [len(i) for i in pred_instances]\n",
    "    mask_probs_pred = mask_probs_pred.split(num_boxes_per_image, dim=0)\n",
    "\n",
    "    for prob, instances in zip(mask_probs_pred, pred_instances):\n",
    "        instances.pred_masks = prob "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "abc19b0f66414e85",
   "metadata": {},
   "source": [
    "重写 ROI Head 的逻辑"
   ]
  },
  {
   "cell_type": "code",
   "id": "580d3cad68adadb7",
   "metadata": {},
   "source": [
    "class MyROIHeads(StandardROIHeads):\n",
    "    def __init__(self, *, box_in_features, box_pooler, box_head, box_predictor: nn.Module, **kwargs):\n",
    "        super().__init__(box_in_features=box_in_features, box_pooler=box_pooler, box_head=box_head,\n",
    "                         box_predictor=box_predictor, **kwargs)\n",
    "        self.projection_adapter = nn.Linear(1024, 512)  # 用于对齐的层\n",
    "\n",
    "    def forward(self, features, proposals, text_embeddings, targets=None):\n",
    "        if self.training:\n",
    "            assert targets, \"'targets' argument is required during training\"\n",
    "            proposals = self.label_and_sample_proposals(proposals, targets)\n",
    "        del targets\n",
    "\n",
    "        if self.training:\n",
    "            losses = self._forward_box(features, proposals, text_embeddings)\n",
    "            losses.update(self._forward_mask(features, proposals))\n",
    "            return proposals, losses\n",
    "        else:\n",
    "            pred_instances, scores = self._forward_box(features, proposals, text_embeddings)\n",
    "            \n",
    "            # 接下来就是self.forward_with_given_boxes的调用，这个是用来求mask和key point的。这里只需要mask，同时必须设定为类型不可知模式，因为此时预测的类别还没有给出。\n",
    "            # 其实稍加改动mask_rcnn_inderence(roi_heads\\mask_head)就可以兼容类型可知模式，这里已exhausted……不管了，直接用原来的\n",
    "            # pred_instances = self._forward_mask(features, pred_instances)  # 显存爆炸，分完box以后再做这一部分\n",
    "            \n",
    "            return pred_instances, scores\n",
    "        \n",
    "    def _forward_box(self, features, proposals, text_embeddings):\n",
    "        features = [features[f] for f in self.box_in_features]\n",
    "        box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])\n",
    "        box_features = self.box_head(box_features)\n",
    "        _, box_deltas = self.box_predictor(box_features)\n",
    "        box_features = self.projection_adapter(box_features)\n",
    "        box_features = F.normalize(box_features, p=2, dim=1)  # 这就是最终的 embedding 了\n",
    "        \n",
    "        scores = box_features @ text_embeddings.T\n",
    "        scores = scores.softmax(-1)\n",
    "\n",
    "        if self.training:\n",
    "            losses = self.box_predictor.losses((scores, box_deltas), proposals)\n",
    "            # proposals is modified in-place below, so losses must be computed first.\n",
    "            if self.train_on_pred_boxes:\n",
    "                with torch.no_grad():\n",
    "                    pred_boxes = self.box_predictor.predict_boxes_for_gt_classes(\n",
    "                        (scores, box_deltas), proposals\n",
    "                    )\n",
    "                    for proposals_per_image, pred_boxes_per_image in zip(proposals, pred_boxes):\n",
    "                        proposals_per_image.proposal_boxes = Boxes(pred_boxes_per_image)\n",
    "            return losses\n",
    "        else:\n",
    "            # 第一步调用boxes = self.predict_boxes(predictions, proposals)，这里把偏移边框实际应用，得到预测框，可以用\n",
    "            boxes = self.box_predictor.predict_boxes((scores, box_deltas), proposals)\n",
    "            for proposals_per_image, boxes_per_image in zip(proposals, boxes):\n",
    "                proposals_per_image.pred_boxes = Boxes(boxes_per_image)\n",
    "                \n",
    "            return proposals, scores  # 这里，分数也需要输出出去。proposals实际是Instances，加上了偏移后的boxes\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def label_and_sample_proposals(self, proposals, targets):\n",
    "        if self.proposal_append_gt:\n",
    "            proposals = add_ground_truth_to_proposals(targets, proposals)\n",
    "\n",
    "        proposals_with_gt = []\n",
    "\n",
    "        num_fg_samples = []\n",
    "        num_bg_samples = []\n",
    "        for proposals_per_image, targets_per_image in zip(proposals, targets):\n",
    "            has_gt = len(targets_per_image) > 0\n",
    "            match_quality_matrix = pairwise_iou(\n",
    "                targets_per_image.gt_boxes, proposals_per_image.proposal_boxes\n",
    "            )\n",
    "            matched_idxs, matched_labels = self.proposal_matcher(match_quality_matrix)\n",
    "            sampled_idxs, gt_classes = self._sample_proposals(\n",
    "                matched_idxs, matched_labels, targets_per_image.gt_classes\n",
    "            )\n",
    "            proposals_per_image = proposals_per_image[sampled_idxs]\n",
    "            proposals_per_image.gt_classes = gt_classes\n",
    "\n",
    "            if has_gt:\n",
    "                sampled_targets = matched_idxs[sampled_idxs]\n",
    "                for (trg_name, trg_value) in targets_per_image.get_fields().items():\n",
    "                    if trg_name.startswith(\"gt_\") and not proposals_per_image.has(trg_name):\n",
    "                        proposals_per_image.set(trg_name, trg_value[sampled_targets])\n",
    "            num_bg_samples.append((gt_classes == self.num_classes).sum().item())\n",
    "            num_fg_samples.append(gt_classes.numel() - num_bg_samples[-1])\n",
    "            proposals_with_gt.append(proposals_per_image)\n",
    "\n",
    "        return proposals_with_gt\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb7d370c2eea5f4a",
   "metadata": {},
   "source": [
    "对 Mask R-CNN 的实现"
   ]
  },
  {
   "cell_type": "code",
   "id": "4d8f27b1d83451f",
   "metadata": {},
   "source": [
    "class MyMRCNN2(GeneralizedRCNN):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "        \n",
    "    def forward(self, image_sizes, clip_feature, text_embeddings, gt_instances=None):\n",
    "        if not self.training:\n",
    "            return self.inference(image_sizes, clip_feature, text_embeddings)\n",
    "        \n",
    "        features = self.backbone(clip_feature)\n",
    "        proposals, proposal_losses = self.proposal_generator(image_sizes, features, gt_instances)  # 这里会产生初始框proposal的损失\n",
    "        _, detector_losses = self.roi_heads(features, proposals, text_embeddings, gt_instances)\n",
    "        \n",
    "        \"\"\"可视化不在这里进行了\n",
    "        if self.vis_period > 0:\n",
    "            storage = get_event_storage()\n",
    "            if storage.iter % self.vis_period == 0:\n",
    "                self.visualize_training(batched_inputs, proposals)\n",
    "        \"\"\"\n",
    "\n",
    "        losses = {}\n",
    "        losses.update(detector_losses)\n",
    "        losses.update(proposal_losses)\n",
    "        return losses\n",
    "        \n",
    "    def inference(self, image_sizes, clip_feature, text_embeddings):\n",
    "        features = self.backbone(clip_feature)\n",
    "        proposals, _ = self.proposal_generator(image_sizes, features)\n",
    "\n",
    "        results, scores = self.roi_heads(features, proposals, text_embeddings)\n",
    "        \n",
    "        return results, scores, features  \n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, cfg, weight_path, device=\"cuda\"):\n",
    "        model = cls(cfg)\n",
    "        checkpointer = DetectionCheckpointer(model)\n",
    "        checkpointer.load(weight_path)\n",
    "        \n",
    "        # fpn 的 bottom_up 改为 CLIP 专属的 Adapter\n",
    "        model.backbone.bottom_up = CLIPtoFPNAdapter()\n",
    "        \n",
    "        # 各类型改为更便利的子类\n",
    "        model.proposal_generator.__class__ = MyRPN\n",
    "        \n",
    "        model.roi_heads.__class__ = MyROIHeads\n",
    "        model.roi_heads.projection_adapter = nn.Linear(1024, 512)  # 还需要添加一个投影来对齐类别特征与文本embeddings\n",
    "        \n",
    "        model.roi_heads.box_predictor.__class__ = MyBoxPredictor\n",
    "        model.roi_heads.box_predictor.cls_score = None  # 移除不需要的参数\n",
    "        \n",
    "        model.roi_heads.mask_head.__class__ = MyMaskHead\n",
    "        \n",
    "        return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f3ff4034b0609afe",
   "metadata": {},
   "source": [
    "CLIP stage 1 的输出无法直接交给FPN，因此需要一个Adapter\n",
    "\n",
    "FPN的实现中有类似作用的定位，即 FPN 的bottom_up，可以用这个来替换\n",
    "\n",
    "使用卷积(kernel_size=1时等同于全连接)+插值来构造多层特征"
   ]
  },
  {
   "cell_type": "code",
   "id": "ac3f7f05c59b7815",
   "metadata": {},
   "source": [
    "class CLIPtoFPNAdapter(nn.Module):\n",
    "    def __init__(self, in_channels=768, out_channels_list=[2048, 1024, 512, 256]):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for out_ch in out_channels_list:\n",
    "            self.convs.append(nn.Conv2d(in_channels, out_ch, kernel_size=1))\n",
    "            \n",
    "        self.apply(init_weights_xavier)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 768, 7, 7]\n",
    "        c5 = self.convs[0](x)  # [B, 256, 7, 7]\n",
    "        c4 = self.convs[1](F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False))  # [B, 256, 14, 14]\n",
    "        c3 = self.convs[2](F.interpolate(x, scale_factor=4, mode='bilinear', align_corners=False))  # [B, 256, 28, 28]\n",
    "        c2 = self.convs[3](F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=False))  # [B, 256, 56, 56]\n",
    "\n",
    "        features = {\n",
    "            \"res2\": c2,\n",
    "            \"res3\": c3,\n",
    "            \"res4\": c4,\n",
    "            \"res5\": c5,\n",
    "        }\n",
    "        return features\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "24fb630a9581b5c3",
   "metadata": {},
   "source": [
    "Dataset和Dataloader总体沿用之前的"
   ]
  },
  {
   "cell_type": "code",
   "id": "117c43e9088ebdf7",
   "metadata": {},
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset_name=None, dataset_dicts=None, meta_data=None, requires_instances=False, half=False):\n",
    "        if dataset_name is None:\n",
    "            if dataset_dicts is None or meta_data is None:\n",
    "                raise ValueError(\"dataset_name为None时dataset_dicts与meta_data不能为None\")\n",
    "            else:\n",
    "                self.dataset_dicts = dataset_dicts\n",
    "                self.meta_data = meta_data\n",
    "        else:\n",
    "            self.dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "            self.meta_data = MetadataCatalog.get(dataset_name)\n",
    "\n",
    "        self.require_instances = requires_instances\n",
    "        self.half = half\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_dicts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.dataset_dicts[idx].copy()\n",
    "        # 读取 cv2 图像\n",
    "        org_img = cv2.imread(d[\"file_name\"])\n",
    "        # d['cv2'] = org_img  # 不需要了\n",
    "        # 这一部分来自defaults的__call__\n",
    "        # 转 tensor\n",
    "        # img = self.aug.get_transform(org_img).apply_image(org_img)\n",
    "        d['image'] = torch.as_tensor(org_img.astype(\"float32\").transpose(2, 0, 1))\n",
    "        if self.half:\n",
    "            d['image'] = d['image'].half()\n",
    "\n",
    "        # 模型还希望在训练时能够有'Instances'，这里也加上\n",
    "        if self.require_instances:\n",
    "            height, width = org_img.shape[:2]\n",
    "            instances = Instances((height, width))\n",
    "            boxes = []\n",
    "            classes = []\n",
    "            masks = []\n",
    "            for ann in d[\"annotations\"]:\n",
    "                x, y, w, h = ann[\"bbox\"]\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "                classes.append(ann[\"category_id\"])\n",
    "                if \"segmentation\" in ann:\n",
    "                    seg = ann[\"segmentation\"]\n",
    "                    if isinstance(seg, list):\n",
    "                        # polygon\n",
    "                        mask = np.zeros((height, width), dtype=np.uint8)\n",
    "                        for poly in seg:\n",
    "                            poly = np.array(poly).reshape((-1, 2))\n",
    "                            cv2.fillPoly(mask, [poly.astype(np.int32)], 1)\n",
    "                        masks.append(mask)\n",
    "                    elif isinstance(seg, dict):\n",
    "                        # RLE\n",
    "                        rle = seg\n",
    "                        if isinstance(rle['counts'], bytes):\n",
    "                            rle['counts'] = rle['counts'].decode('utf-8')  # 如果是 bytes\n",
    "                        mask = mask_utils.decode(rle)\n",
    "                        masks.append(mask.astype(np.uint8))\n",
    "\n",
    "            if self.half:\n",
    "                instances.gt_boxes = Boxes(torch.tensor(boxes, dtype=torch.float32).half())\n",
    "                instances.gt_classes = torch.tensor(classes, dtype=torch.int64).half()\n",
    "                instances.gt_masks = BitMasks(torch.tensor(np.stack(masks, axis=0), dtype=torch.uint8).half())\n",
    "            else:\n",
    "                instances.gt_boxes = Boxes(torch.tensor(boxes, dtype=torch.float32))\n",
    "                instances.gt_classes = torch.tensor(classes, dtype=torch.int64)\n",
    "                instances.gt_masks = BitMasks(torch.tensor(np.stack(masks, axis=0), dtype=torch.uint8))\n",
    "            d[\"instances\"] = instances\n",
    "\n",
    "        return d\n",
    "\n",
    "\n",
    "def build_batch_loader(dataset_name=None, dataset_dicts=None, meta_data=None, batch_size=1, shuffle=False,\n",
    "                       requires_instances=False, half=False):\n",
    "    if dataset_name is None:\n",
    "        if dataset_dicts is None or meta_data is None:\n",
    "            raise ValueError(\"dataset_name为None时dataset_dicts与meta_data不能为None\")\n",
    "        else:\n",
    "            dataset = MyDataset(dataset_dicts=dataset_dicts, meta_data=meta_data, requires_instances=requires_instances, half=half)\n",
    "    else:\n",
    "        dataset = MyDataset(dataset_name=dataset_name, requires_instances=requires_instances, half=half)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: batch\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe4794df02b691a5",
   "metadata": {},
   "source": [
    "class MyFVLM(nn.Module):\n",
    "    def __init__(self, clip_path, cfg, mask_rcnn_weight_path, device=\"cpu\", top_k=100, nms_thresh=0.5, score_thresh=0.05):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.clip_model = TwoStageCLIPModel.from_pretrained(clip_path).to(device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(clip_path)\n",
    "        self.mask_rcnn_model = MyMRCNN2.from_pretrained(cfg, mask_rcnn_weight_path).to(device)\n",
    "        \n",
    "        self.mask_rcnn_model.roi_heads.box_predictor.test_topk_per_image = top_k\n",
    "        self.mask_rcnn_model.roi_heads.box_predictor.test_nms_thresh = nms_thresh\n",
    "        self.mask_rcnn_model.roi_heads.box_predictor.test_score_thresh = score_thresh\n",
    "\n",
    "        self.froze_VLM()\n",
    "\n",
    "    def froze_VLM(self):\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, batched_inputs, text_embeddings):\n",
    "        if not self.training:\n",
    "            return self.inference(batched_inputs, text_embeddings)\n",
    "\n",
    "        batch_imgs = [bat[\"image\"] for bat in batched_inputs]\n",
    "\n",
    "        post_imgs = self.clip_processor(images=batch_imgs, return_tensors='pt', padding=True).to(\n",
    "            self.device)  # TODO: 训练前做好是否可能？\n",
    "\n",
    "        orig_size = post_imgs[\"pixel_values\"].shape[-1]\n",
    "\n",
    "        clip_feature = self.clip_model.get_image_features_stage1(post_imgs[\"pixel_values\"])\n",
    "        clip_feature = clip_feature.permute(0, 2, 1)\n",
    "        clip_feature = clip_feature[:, :, 1:]\n",
    "        last_dim_len = int(np.sqrt(clip_feature.shape[2]))\n",
    "        clip_feature = clip_feature.reshape(clip_feature.shape[0], clip_feature.shape[1], last_dim_len, last_dim_len)\n",
    "\n",
    "        gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n",
    "\n",
    "        image_sizes = [(orig_size, orig_size) for bat in batched_inputs]\n",
    "\n",
    "        loss_dict = self.mask_rcnn_model(image_sizes, clip_feature, text_embeddings, gt_instances)\n",
    "\n",
    "        return loss_dict\n",
    "\n",
    "    def inference(self, batched_inputs, text_embeddings):\n",
    "        batch_imgs = [bat[\"image\"] for bat in batched_inputs]\n",
    "\n",
    "        post_imgs = self.clip_processor(images=batch_imgs, return_tensors='pt', padding=True).to(\n",
    "            self.device)  # TODO: 训练前做好是否可能？\n",
    "\n",
    "        orig_size = post_imgs[\"pixel_values\"].shape[-1]\n",
    "\n",
    "        clip_feature = self.clip_model.get_image_features_stage1(post_imgs[\"pixel_values\"])\n",
    "        clip_feature = clip_feature.permute(0, 2, 1)\n",
    "        clip_feature = clip_feature[:, :, 1:]\n",
    "        last_dim_len = int(np.sqrt(clip_feature.shape[2]))\n",
    "        clip_feature = clip_feature.reshape(clip_feature.shape[0], clip_feature.shape[1], last_dim_len, last_dim_len)\n",
    "\n",
    "        image_sizes = [(orig_size, orig_size) for bat in batched_inputs]  # image sizes应该选取模型来自的图\n",
    "\n",
    "        results, scores_1, backbone_features = self.mask_rcnn_model(image_sizes, clip_feature, text_embeddings)\n",
    "\n",
    "        boxes_list = [proposal.pred_boxes for proposal in results]\n",
    "        pooler_fmt_boxes = convert_boxes_to_pooler_format(boxes_list)\n",
    "        del boxes_list\n",
    "\n",
    "        # clip_feature可以作为Top-Level Feature Map直接进行ROI Align。原图尺寸也要取orig_size\n",
    "        roi_align = ROIAlign(output_size=(1, 1), spatial_scale=last_dim_len / orig_size,\n",
    "                             sampling_ratio=2, aligned=True).to(self.device)\n",
    "        clip_feature = roi_align(clip_feature, pooler_fmt_boxes)[:, :, 0, 0]  # 获得逐个proposal的embedding\n",
    "        clip_feature = self.clip_model.get_image_features_stage2(clip_feature)  # 加入最后的池化\n",
    "\n",
    "        scores_2 = clip_feature @ text_embeddings.T  # 计算得到第二个分数\n",
    "        scores_2 = scores_2.softmax(dim=-1)\n",
    "        scores = torch.sqrt(scores_1 * scores_2)  # 终于有scores了，现在需要把之前缺少的东西补齐\n",
    "        scores = scores.softmax(dim=-1)\n",
    "\n",
    "        results = self.post_process_with_scores(results, scores, backbone_features, [(bat[\"image\"].shape[-2], bat[\"image\"].shape[-1]) for bat in batched_inputs])\n",
    "        return results\n",
    "\n",
    "    def post_process_with_scores(self, instances, scores, backbone_features, image_sizes):\n",
    "        results, _ = fast_rcnn_inference(\n",
    "            [p.pred_boxes.tensor for p in instances],\n",
    "            scores.split([len(p.pred_boxes) for p in instances], dim=0),\n",
    "            [p.image_size for p in instances],\n",
    "            self.mask_rcnn_model.roi_heads.box_predictor.test_score_thresh,\n",
    "            self.mask_rcnn_model.roi_heads.box_predictor.test_nms_thresh,\n",
    "            self.mask_rcnn_model.roi_heads.box_predictor.test_topk_per_image\n",
    "        )\n",
    "        results = self.mask_rcnn_model.roi_heads._forward_mask(backbone_features, results)\n",
    "\n",
    "        processed_results = []\n",
    "        for results_per_image, img_size in zip(results, image_sizes):\n",
    "            r = detector_postprocess(results_per_image, img_size[0], img_size[1])\n",
    "            processed_results.append({\"instances\": r})\n",
    "\n",
    "        return processed_results\n",
    "\n",
    "    def class_name_list_prepare(self, class_name_list):\n",
    "        # class_name_list = [\"a photo of \" + cls for cls in class_name_list]  # TODO: zero-shot 测试？\n",
    "        class_name_list.append(\"background or no object\")  # TODO: 确认background怎么弄\n",
    "        return class_name_list\n",
    "\n",
    "    def get_cls_embedding(self, class_name_list):\n",
    "        class_inputs = self.clip_processor(text=class_name_list, return_tensors=\"pt\", padding=True).to(\n",
    "            self.clip_model.device)\n",
    "        return self.clip_model.get_text_features(**class_inputs).to(self.clip_model.device)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({\"model\": self.mask_rcnn_model.state_dict()}, path)\n",
    "\n",
    "    def load(self, path, device):\n",
    "        self.device = device\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.mask_rcnn_model.load_state_dict(checkpoint[\"model\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "620d7b43d5b1afd8",
   "metadata": {},
   "source": [
    "定义训练保存机制"
   ]
  },
  {
   "cell_type": "code",
   "id": "766db088a6a8198f",
   "metadata": {},
   "source": [
    "def save_training_state(model, optimizer, scheduler, scaler, epoch, path):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.mask_rcnn_model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict() if scheduler else None,\n",
    "        \"scaler\": scaler.state_dict() if scaler else None,\n",
    "    }, path)\n",
    "\n",
    "\n",
    "def load_training_state(model, optimizer, scheduler, scaler, path, device=\"cuda\"):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    if checkpoint.get(\"model\") and model.mask_rcnn_model:\n",
    "        model.mask_rcnn_model.load_state_dict(checkpoint[\"model\"])\n",
    "    if checkpoint.get(\"optimizer\") and optimizer:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if checkpoint.get(\"scheduler\") and scheduler:\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "    if checkpoint.get(\"scaler\") and scaler:\n",
    "        scheduler.load_state_dict(checkpoint[\"scaler\"])\n",
    "    epoch = checkpoint.get(\"epoch\", 0)\n",
    "    return epoch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b99a6768db4c4ddf",
   "metadata": {},
   "source": [
    "固定种子或加速计算"
   ]
  },
  {
   "cell_type": "code",
   "id": "494b84f0be0bdc36",
   "metadata": {},
   "source": [
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc1e7e862dc2869",
   "metadata": {},
   "source": [
    "if deterministic:\n",
    "    set_seed(0)\n",
    "else:\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c2cc5db484c9dabd",
   "metadata": {},
   "source": [
    "实现训练函数"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca67bafc38ca4ee",
   "metadata": {},
   "source": [
    "def train(model, optimizer, scheduler, epoch_num, batch_size, dataset_name, dataset_path, dataset_json, shuffle=False,\n",
    "          save_epoch=0, train_name=\"test\", half=False):\n",
    "    if dataset_name not in DatasetCatalog.list():\n",
    "        register_coco_instances(dataset_name, {}, dataset_json, dataset_path)\n",
    "    dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "    meta_data = MetadataCatalog.get(dataset_name)\n",
    "\n",
    "    loader = build_batch_loader(dataset_dicts=dataset_dicts, meta_data=meta_data,\n",
    "                                batch_size=batch_size, shuffle=shuffle, requires_instances=True, half=half)\n",
    "\n",
    "    autocast_ctx = autocast(dtype=torch.float16) if half else nullcontext()\n",
    "\n",
    "    with torch.no_grad(), autocast_ctx:\n",
    "        class_name_list = model.class_name_list_prepare(meta_data.thing_classes)\n",
    "        class_embeddings = model.get_cls_embedding(class_name_list)\n",
    "\n",
    "    all_epoch_losses = []  # 保存每个epoch的平均loss\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss_sum = {}\n",
    "\n",
    "        cnt = 0\n",
    "\n",
    "        for batch in loader:\n",
    "            with autocast_ctx:\n",
    "                losses = model(batch, class_embeddings)\n",
    "                total_loss = sum(losses.values())\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                for k, v in losses.items():\n",
    "                    epoch_loss_sum[k] = epoch_loss_sum.get(k, 0.0) + v.item()\n",
    "\n",
    "                cnt += 1\n",
    "                if cnt % 20 == 0:\n",
    "                    # 获取当前学习率（假设optimizer只有一个参数组）\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "                    print(f\"Epoch {epoch}/{epoch_num}, Batch {cnt}/{len(loader)}, lr {current_lr}\")\n",
    "\n",
    "                    epoch_loss_avg = {k: v / len(loader) for k, v in epoch_loss_sum.items()}\n",
    "                    all_epoch_losses.append(epoch_loss_avg)\n",
    "\n",
    "                    # 计算总平均 loss\n",
    "                    total_epoch_loss = sum(epoch_loss_avg.values())\n",
    "\n",
    "                    # 打印信息\n",
    "                    for k, v in epoch_loss_avg.items():\n",
    "                        print(f\"  {k}: {v:.4f}\")\n",
    "                    print(f\"  Total Loss: {total_epoch_loss:.4f}\\n\")\n",
    "\n",
    "                    epoch_loss_sum = {}\n",
    "        \n",
    "                    scheduler.step()  # 训练轮次少，因此每20个batch就更新\n",
    "\n",
    "        if epoch % save_epoch == 0:\n",
    "            save_training_state(model, optimizer, scheduler, epoch, f\"../model/checkpoint/{train_name}_{dataset_name}_{epoch}.pth\")\n",
    "            print(f\"Saved checkpoint for epoch {epoch + 1}\")\n",
    "            \n",
    "        # 清空输出并重新显示\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        for key in epoch_loss_avg.keys():\n",
    "            ax.plot(range(1, len(all_epoch_losses)+1),\n",
    "                    [e[key] for e in all_epoch_losses],\n",
    "                    marker=\"o\",\n",
    "                    label=key)\n",
    "        ax.set_title(f\"Training Losses (Epoch {epoch+1}/{epoch_num})\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss Value\")\n",
    "        ax.set_xlim(1, epoch_num)\n",
    "        ax.set_ylim(bottom=0)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "            \n",
    "            \n",
    "    \"\"\"        \n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    for key in epoch_loss_avg.keys():\n",
    "        ax.plot(range(1, len(all_epoch_losses)+1),\n",
    "                [e[key] for e in all_epoch_losses],\n",
    "                marker=\"o\",\n",
    "                label=key)\n",
    "        \n",
    "    ax.set_title(f\"Training Losses\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss Value\")\n",
    "    ax.set_xlim(1, epoch_num)\n",
    "    ax.set_ylim(bottom=1e-8)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    display(fig)\n",
    "    \"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5af1817a6e4eaf98",
   "metadata": {},
   "source": [
    "实例化model"
   ]
  },
  {
   "cell_type": "code",
   "id": "832f0a5bdf86a33c",
   "metadata": {},
   "source": "model = MyFVLM(CLIP_PATH, cfg, MyMRCNN_CLSFREE_PATH, \"cuda\");",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9e14b5cd8f9335aa",
   "metadata": {},
   "source": [
    "实例化optimizer与scheduler"
   ]
  },
  {
   "cell_type": "code",
   "id": "caee611f03127c33",
   "metadata": {},
   "source": [
    "optimizer = torch.optim.AdamW(model.mask_rcnn_model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = cosine_scheduler_with_warmup(optimizer, warmup_steps=500, total_steps=10000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1375c110b701844f",
   "metadata": {},
   "source": [
    "需要加载的话"
   ]
  },
  {
   "cell_type": "code",
   "id": "189654216c6e988c",
   "metadata": {},
   "source": [
    "#checkpoint_path = \"\"\n",
    "#load_training_state(model, optimizer, scheduler, checkpoint_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "92dce5b9b4865f14",
   "metadata": {},
   "source": [
    "实际训练"
   ]
  },
  {
   "cell_type": "code",
   "id": "949dac3941c47364",
   "metadata": {},
   "source": [
    "train(model, optimizer, scheduler, 100, 20, \"complete_train\",\n",
    "      train_images, filtered_train_json, False, 5, \"test\", True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "实现测试函数",
   "id": "388437450ea149c"
  },
  {
   "cell_type": "code",
   "id": "1475075910e07ffc",
   "metadata": {},
   "source": [
    "@torch.no_grad()\n",
    "def test(model, batch_size, dataset_name, dataset_path, dataset_json, model_load_path=None, evaluator_output_path=None,\n",
    "         shuffle=False, half=False, visualize_=False, visualize_path=None):\n",
    "    if dataset_name not in DatasetCatalog.list():\n",
    "        register_coco_instances(dataset_name, {}, dataset_json, dataset_path)\n",
    "    dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "    meta_data = MetadataCatalog.get(dataset_name)\n",
    "\n",
    "    loader = build_batch_loader(dataset_dicts=dataset_dicts, meta_data=meta_data,\n",
    "                                batch_size=batch_size, shuffle=shuffle, requires_instances=True, half=half)\n",
    "\n",
    "    autocast_ctx = autocast(dtype=torch.float16) if half else nullcontext()\n",
    "\n",
    "    evaluator = COCOEvaluator(dataset_name if dataset_name else \"custom_coco\", output_dir=evaluator_output_path)\n",
    "    evaluator.reset()\n",
    "\n",
    "    if model_load_path:\n",
    "        model.load(model_load_path, model.device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad(), autocast_ctx:\n",
    "        class_name_list = model.class_name_list_prepare(meta_data.thing_classes)\n",
    "        class_embeddings = model.get_cls_embedding(class_name_list)\n",
    "        class_embeddings = class_embeddings.half()  # 确保是半精度张量\n",
    "\n",
    "\n",
    "    for batch in loader:\n",
    "        results = []  # 不要一次性存太多results了\n",
    "        with torch.no_grad(), autocast_ctx:\n",
    "            pred_instances = model.inference(batch, class_embeddings)\n",
    "\n",
    "            # 转换为 Detectron2 标准格式\n",
    "            for det, inp in zip(pred_instances, batch):\n",
    "                out_dict = {\n",
    "                    \"image_id\": inp[\"image_id\"],\n",
    "                    \"instances\": det[\"instances\"].to(\"cpu\"),\n",
    "                    \"height\": inp[\"height\"],\n",
    "                    \"width\": inp[\"width\"],\n",
    "                    \"image\": inp[\"image\"].cpu(),\n",
    "                    \"file_name\": inp[\"file_name\"]\n",
    "                }\n",
    "                evaluator.process([inp], [out_dict])\n",
    "                results.append(out_dict)\n",
    "            del pred_instances, batch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if visualize_:\n",
    "                visualize(results, visualize_path, meta_data)\n",
    "\n",
    "    metrics = evaluator.evaluate()\n",
    "    return metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "定义可视化函数",
   "id": "19ba5c620aa7b4ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize(results, visualize_path, meta_data):\n",
    "    for result in results:\n",
    "        img = result[\"cv2\"][:, :, ::-1]\n",
    "\n",
    "        # 创建 Visualizer\n",
    "        v = Visualizer(img, metadata=meta_data, scale=1.2)\n",
    "\n",
    "        out = v.draw_instance_predictions(result[\"instance\"])\n",
    "        img_vis = out.get_image()\n",
    "\n",
    "        img_path = os.path.join(visualize_path, f\"{os.path.basename(result['file_name'])}\")\n",
    "        if not cv2.imwrite(img_path, img_vis[..., ::-1]):  # 转回 BGR\n",
    "            raise IOError(f\"Failed to visualize image {img_path}\")\n",
    "        print(f\"saved at {img_path}\")"
   ],
   "id": "ec999f0c85d51fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = MyFVLM(CLIP_PATH, cfg, MyMRCNN_CLSFREE_PATH, \"cuda\");",
   "id": "ccc513f89adbbf3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test(model, 15, \"val_test\", val_images, filtered_val_json, model_load_path=\"../model/checkpoint/test_complete_train_5.pth\",\n",
    "     evaluator_output_path=\"../output/output_test/eval/\", shuffle=False, half=half, visualize_=True, visualize_path=\"../output/output_test/img/\")"
   ],
   "id": "f995710b60720c9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4b2f903b1b0d343a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
