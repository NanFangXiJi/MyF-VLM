{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "本方案直接对每个回归头输出的proposal裁剪出图像块，输入CLIP\n",
    "\n",
    "本方案不需要额外的训练"
   ],
   "id": "bcc7e58bce2c805d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:17:27.535528Z",
     "start_time": "2025-10-15T05:17:23.429566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "test_mode = True"
   ],
   "id": "e6bdf448647e1fa7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\SomeRandomTask\\CLIP-MRNN\\.venv\\lib\\site-packages\\detectron2\\model_zoo\\model_zoo.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "首先读取Mask R-CNN，并将其分类头去掉",
   "id": "a3e84ead1cbc01"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-15T05:17:27.566767Z",
     "start_time": "2025-10-15T05:17:27.549439Z"
    }
   },
   "source": [
    "MRCNN_PATH = \"../model/model_final_f10217.pkl\"\n",
    "CONFIG_FILE = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:17:27.596954Z",
     "start_time": "2025-10-15T05:17:27.568765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE))\n",
    "cfg.MODEL.WEIGHTS = MRCNN_PATH\n",
    "cfg.MODEL.DEVICE = \"cuda\""
   ],
   "id": "a95b8de601dcb79d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:17:28.166344Z",
     "start_time": "2025-10-15T05:17:27.598939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask_mrcnn_model = build_model(cfg)\n",
    "mask_mrcnn_model.eval()\n",
    "DetectionCheckpointer(mask_mrcnn_model).load(cfg.MODEL.WEIGHTS)"
   ],
   "id": "504b73d75253fd32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__author__': 'Detectron2 Model Zoo'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:17:28.181780Z",
     "start_time": "2025-10-15T05:17:28.168342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    for name, param in mask_mrcnn_model.named_parameters():\n",
    "        print(name, param.shape)"
   ],
   "id": "4c43f72691740450",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.fpn_lateral2.weight torch.Size([256, 256, 1, 1])\n",
      "backbone.fpn_lateral2.bias torch.Size([256])\n",
      "backbone.fpn_output2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.fpn_output2.bias torch.Size([256])\n",
      "backbone.fpn_lateral3.weight torch.Size([256, 512, 1, 1])\n",
      "backbone.fpn_lateral3.bias torch.Size([256])\n",
      "backbone.fpn_output3.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.fpn_output3.bias torch.Size([256])\n",
      "backbone.fpn_lateral4.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.fpn_lateral4.bias torch.Size([256])\n",
      "backbone.fpn_output4.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.fpn_output4.bias torch.Size([256])\n",
      "backbone.fpn_lateral5.weight torch.Size([256, 2048, 1, 1])\n",
      "backbone.fpn_lateral5.bias torch.Size([256])\n",
      "backbone.fpn_output5.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.fpn_output5.bias torch.Size([256])\n",
      "backbone.bottom_up.stem.conv1.weight torch.Size([64, 3, 7, 7])\n",
      "backbone.bottom_up.res2.0.shortcut.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.bottom_up.res2.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "backbone.bottom_up.res2.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.bottom_up.res2.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.bottom_up.res2.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "backbone.bottom_up.res2.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.bottom_up.res2.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.bottom_up.res2.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "backbone.bottom_up.res2.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.bottom_up.res2.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.bottom_up.res3.0.shortcut.weight torch.Size([512, 256, 1, 1])\n",
      "backbone.bottom_up.res3.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "backbone.bottom_up.res3.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.bottom_up.res3.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.bottom_up.res3.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.bottom_up.res3.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.bottom_up.res3.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.bottom_up.res3.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.bottom_up.res3.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.bottom_up.res3.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.bottom_up.res3.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.bottom_up.res3.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.bottom_up.res3.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.bottom_up.res4.0.shortcut.weight torch.Size([1024, 512, 1, 1])\n",
      "backbone.bottom_up.res4.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "backbone.bottom_up.res4.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res5.0.shortcut.weight torch.Size([2048, 1024, 1, 1])\n",
      "backbone.bottom_up.res5.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "backbone.bottom_up.res5.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.bottom_up.res5.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "backbone.bottom_up.res5.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "backbone.bottom_up.res5.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.bottom_up.res5.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "backbone.bottom_up.res5.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "backbone.bottom_up.res5.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.bottom_up.res5.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "proposal_generator.rpn_head.conv.weight torch.Size([256, 256, 3, 3])\n",
      "proposal_generator.rpn_head.conv.bias torch.Size([256])\n",
      "proposal_generator.rpn_head.objectness_logits.weight torch.Size([3, 256, 1, 1])\n",
      "proposal_generator.rpn_head.objectness_logits.bias torch.Size([3])\n",
      "proposal_generator.rpn_head.anchor_deltas.weight torch.Size([12, 256, 1, 1])\n",
      "proposal_generator.rpn_head.anchor_deltas.bias torch.Size([12])\n",
      "roi_heads.box_head.fc1.weight torch.Size([1024, 12544])\n",
      "roi_heads.box_head.fc1.bias torch.Size([1024])\n",
      "roi_heads.box_head.fc2.weight torch.Size([1024, 1024])\n",
      "roi_heads.box_head.fc2.bias torch.Size([1024])\n",
      "roi_heads.box_predictor.cls_score.weight torch.Size([81, 1024])\n",
      "roi_heads.box_predictor.cls_score.bias torch.Size([81])\n",
      "roi_heads.box_predictor.bbox_pred.weight torch.Size([320, 1024])\n",
      "roi_heads.box_predictor.bbox_pred.bias torch.Size([320])\n",
      "roi_heads.mask_head.mask_fcn1.weight torch.Size([256, 256, 3, 3])\n",
      "roi_heads.mask_head.mask_fcn1.bias torch.Size([256])\n",
      "roi_heads.mask_head.mask_fcn2.weight torch.Size([256, 256, 3, 3])\n",
      "roi_heads.mask_head.mask_fcn2.bias torch.Size([256])\n",
      "roi_heads.mask_head.mask_fcn3.weight torch.Size([256, 256, 3, 3])\n",
      "roi_heads.mask_head.mask_fcn3.bias torch.Size([256])\n",
      "roi_heads.mask_head.mask_fcn4.weight torch.Size([256, 256, 3, 3])\n",
      "roi_heads.mask_head.mask_fcn4.bias torch.Size([256])\n",
      "roi_heads.mask_head.deconv.weight torch.Size([256, 256, 2, 2])\n",
      "roi_heads.mask_head.deconv.bias torch.Size([256])\n",
      "roi_heads.mask_head.predictor.weight torch.Size([80, 256, 1, 1])\n",
      "roi_heads.mask_head.predictor.bias torch.Size([80])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "去掉：\n",
    "\n",
    "roi_heads.box_predictor.cls_score.weight torch.Size([81, 1024])\n",
    "\n",
    "roi_heads.box_predictor.cls_score.bias torch.Size([81])"
   ],
   "id": "6d419bb43a7cfe83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "构建MyMaskRCNN类来实现",
   "id": "55db1f8c00afbacf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:17:31.169656Z",
     "start_time": "2025-10-15T05:17:31.157726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyMaskRCNN(nn.Module):\n",
    "    def __init__(self, cfg=None, original_model=None):\n",
    "        \"\"\"\n",
    "        若提供 original_model：直接复制结构并去掉分类头；\n",
    "        若提供 cfg：根据配置文件构造同结构模型，再去掉分类头；\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if original_model is not None:\n",
    "        #     print(\"[MyMaskRCNN] 从原始模型复制结构。\")\n",
    "            self.backbone = original_model.backbone\n",
    "            self.proposal_generator = original_model.proposal_generator\n",
    "            self.roi_heads = original_model.roi_heads\n",
    "        elif cfg is not None:\n",
    "        #     print(\"[MyMaskRCNN] 根据 cfg 创建空模型结构。\")\n",
    "            model = build_model(cfg)\n",
    "            self.backbone = model.backbone\n",
    "            self.proposal_generator = model.proposal_generator\n",
    "            self.roi_heads = model.roi_heads\n",
    "        else:\n",
    "            raise ValueError(\"必须提供 original_model 或 cfg 之一。\")\n",
    "        \n",
    "        # 去掉不需要的分类头\n",
    "        if hasattr(self.roi_heads, \"box_predictor\"):\n",
    "            box_predictor = self.roi_heads.box_predictor\n",
    "            if hasattr(box_predictor, \"cls_score\"):\n",
    "                del box_predictor.cls_score\n",
    "                box_predictor.cls_score = None\n",
    "\n",
    "    def save_to_pkl(self, path):\n",
    "        \"\"\"\n",
    "        将模型参数保存为.pkl文件\n",
    "        \"\"\"\n",
    "        state_dict = self.state_dict()\n",
    "        data = {\n",
    "            \"model\": state_dict,\n",
    "            \"__author__\": \"Task 1\",\n",
    "            \"matching_heuristics\": True\n",
    "        }\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        # print(f\"[MyMaskRCNN] 参数已保存到 {path}\")\n",
    "\n",
    "    def load_from_pkl(self, path):\n",
    "        \"\"\"\n",
    "        从.pkl文件加载模型参数\n",
    "        \"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        state_dict = data[\"model\"] if \"model\" in data else data\n",
    "        missing, unexpected = self.load_state_dict(state_dict, strict=False)\n",
    "        # print(f\"[MyMaskRCNN] 从 {path} 加载完成。\")\n",
    "        # if missing:\n",
    "        #     print(\"未加载参数：\", missing)\n",
    "        # if unexpected:\n",
    "        #     print(\"未使用参数：\", unexpected)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n"
   ],
   "id": "9861f5278994dbe0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:19:00.408277Z",
     "start_time": "2025-10-15T05:19:00.098322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MyMRCNN_PATH = \"../model/my_mask_rcnn.pkl\"\n",
    "\n",
    "my_mask_rcnn_model = MyMaskRCNN(original_model=mask_mrcnn_model)\n",
    "my_mask_rcnn_model.eval()\n",
    "my_mask_rcnn_model.save_to_pkl(MyMRCNN_PATH)"
   ],
   "id": "7995ceabc85e5faf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MyMaskRCNN] 从原始模型复制结构。\n",
      "[MyMaskRCNN] 参数已保存到 ../model/my_mask_mrcnn.pkl\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试",
   "id": "7fb798d0cd9e272b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:19:12.203396Z",
     "start_time": "2025-10-15T05:19:12.191369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    for name, param in my_mask_rcnn_model.named_parameters():\n",
    "        print(name, param.shape)"
   ],
   "id": "e18131c177dada5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.fpn_lateral2.weight torch.Size([256, 256, 1, 1])\n",
      "backbone.fpn_lateral2.bias torch.Size([256])\n",
      "backbone.fpn_output2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.fpn_output2.bias torch.Size([256])\n",
      "backbone.fpn_lateral3.weight torch.Size([256, 512, 1, 1])\n",
      "backbone.fpn_lateral3.bias torch.Size([256])\n",
      "backbone.fpn_output3.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.fpn_output3.bias torch.Size([256])\n",
      "backbone.fpn_lateral4.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.fpn_lateral4.bias torch.Size([256])\n",
      "backbone.fpn_output4.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.fpn_output4.bias torch.Size([256])\n",
      "backbone.fpn_lateral5.weight torch.Size([256, 2048, 1, 1])\n",
      "backbone.fpn_lateral5.bias torch.Size([256])\n",
      "backbone.fpn_output5.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.fpn_output5.bias torch.Size([256])\n",
      "backbone.bottom_up.stem.conv1.weight torch.Size([64, 3, 7, 7])\n",
      "backbone.bottom_up.res2.0.shortcut.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.bottom_up.res2.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "backbone.bottom_up.res2.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.bottom_up.res2.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.bottom_up.res2.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "backbone.bottom_up.res2.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.bottom_up.res2.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.bottom_up.res2.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "backbone.bottom_up.res2.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.bottom_up.res2.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.bottom_up.res3.0.shortcut.weight torch.Size([512, 256, 1, 1])\n",
      "backbone.bottom_up.res3.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "backbone.bottom_up.res3.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.bottom_up.res3.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.bottom_up.res3.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.bottom_up.res3.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.bottom_up.res3.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.bottom_up.res3.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.bottom_up.res3.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.bottom_up.res3.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.bottom_up.res3.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.bottom_up.res3.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.bottom_up.res3.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.bottom_up.res4.0.shortcut.weight torch.Size([1024, 512, 1, 1])\n",
      "backbone.bottom_up.res4.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "backbone.bottom_up.res4.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res5.0.shortcut.weight torch.Size([2048, 1024, 1, 1])\n",
      "backbone.bottom_up.res5.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "backbone.bottom_up.res5.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.bottom_up.res5.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "backbone.bottom_up.res5.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "backbone.bottom_up.res5.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.bottom_up.res5.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "backbone.bottom_up.res5.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "backbone.bottom_up.res5.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.bottom_up.res5.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "proposal_generator.rpn_head.conv.weight torch.Size([256, 256, 3, 3])\n",
      "proposal_generator.rpn_head.conv.bias torch.Size([256])\n",
      "proposal_generator.rpn_head.objectness_logits.weight torch.Size([3, 256, 1, 1])\n",
      "proposal_generator.rpn_head.objectness_logits.bias torch.Size([3])\n",
      "proposal_generator.rpn_head.anchor_deltas.weight torch.Size([12, 256, 1, 1])\n",
      "proposal_generator.rpn_head.anchor_deltas.bias torch.Size([12])\n",
      "roi_heads.box_head.fc1.weight torch.Size([1024, 12544])\n",
      "roi_heads.box_head.fc1.bias torch.Size([1024])\n",
      "roi_heads.box_head.fc2.weight torch.Size([1024, 1024])\n",
      "roi_heads.box_head.fc2.bias torch.Size([1024])\n",
      "roi_heads.box_predictor.bbox_pred.weight torch.Size([320, 1024])\n",
      "roi_heads.box_predictor.bbox_pred.bias torch.Size([320])\n",
      "roi_heads.mask_head.mask_fcn1.weight torch.Size([256, 256, 3, 3])\n",
      "roi_heads.mask_head.mask_fcn1.bias torch.Size([256])\n",
      "roi_heads.mask_head.mask_fcn2.weight torch.Size([256, 256, 3, 3])\n",
      "roi_heads.mask_head.mask_fcn2.bias torch.Size([256])\n",
      "roi_heads.mask_head.mask_fcn3.weight torch.Size([256, 256, 3, 3])\n",
      "roi_heads.mask_head.mask_fcn3.bias torch.Size([256])\n",
      "roi_heads.mask_head.mask_fcn4.weight torch.Size([256, 256, 3, 3])\n",
      "roi_heads.mask_head.mask_fcn4.bias torch.Size([256])\n",
      "roi_heads.mask_head.deconv.weight torch.Size([256, 256, 2, 2])\n",
      "roi_heads.mask_head.deconv.bias torch.Size([256])\n",
      "roi_heads.mask_head.predictor.weight torch.Size([80, 256, 1, 1])\n",
      "roi_heads.mask_head.predictor.bias torch.Size([80])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T05:19:35.758131Z",
     "start_time": "2025-10-15T05:19:34.995491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    my_mask_rcnn_model = MyMaskRCNN(cfg=cfg)\n",
    "    my_mask_rcnn_model.load_from_pkl(MyMRCNN_PATH)\n",
    "    for name, param in my_mask_rcnn_model.named_parameters():\n",
    "        print(name, param.shape)"
   ],
   "id": "db2e9b2ea5b67f44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MyMaskRCNN] 根据 cfg 创建空模型结构。\n",
      "[MyMaskRCNN] 从 ../model/my_mask_mrcnn.pkl 加载完成。\n",
      "backbone.fpn_lateral2.weight torch.Size([256, 256, 1, 1])\n",
      "backbone.fpn_lateral2.bias torch.Size([256])\n",
      "backbone.fpn_output2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.fpn_output2.bias torch.Size([256])\n",
      "backbone.fpn_lateral3.weight torch.Size([256, 512, 1, 1])\n",
      "backbone.fpn_lateral3.bias torch.Size([256])\n",
      "backbone.fpn_output3.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.fpn_output3.bias torch.Size([256])\n",
      "backbone.fpn_lateral4.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.fpn_lateral4.bias torch.Size([256])\n",
      "backbone.fpn_output4.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.fpn_output4.bias torch.Size([256])\n",
      "backbone.fpn_lateral5.weight torch.Size([256, 2048, 1, 1])\n",
      "backbone.fpn_lateral5.bias torch.Size([256])\n",
      "backbone.fpn_output5.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.fpn_output5.bias torch.Size([256])\n",
      "backbone.bottom_up.stem.conv1.weight torch.Size([64, 3, 7, 7])\n",
      "backbone.bottom_up.res2.0.shortcut.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.bottom_up.res2.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "backbone.bottom_up.res2.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.bottom_up.res2.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.bottom_up.res2.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "backbone.bottom_up.res2.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.bottom_up.res2.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.bottom_up.res2.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "backbone.bottom_up.res2.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "backbone.bottom_up.res2.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "backbone.bottom_up.res3.0.shortcut.weight torch.Size([512, 256, 1, 1])\n",
      "backbone.bottom_up.res3.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "backbone.bottom_up.res3.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.bottom_up.res3.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.bottom_up.res3.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.bottom_up.res3.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.bottom_up.res3.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.bottom_up.res3.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.bottom_up.res3.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.bottom_up.res3.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.bottom_up.res3.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "backbone.bottom_up.res3.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "backbone.bottom_up.res3.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "backbone.bottom_up.res4.0.shortcut.weight torch.Size([1024, 512, 1, 1])\n",
      "backbone.bottom_up.res4.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "backbone.bottom_up.res4.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res4.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "backbone.bottom_up.res4.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "backbone.bottom_up.res4.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "backbone.bottom_up.res5.0.shortcut.weight torch.Size([2048, 1024, 1, 1])\n",
      "backbone.bottom_up.res5.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "backbone.bottom_up.res5.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.bottom_up.res5.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "backbone.bottom_up.res5.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "backbone.bottom_up.res5.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.bottom_up.res5.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "backbone.bottom_up.res5.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "backbone.bottom_up.res5.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "backbone.bottom_up.res5.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "proposal_generator.rpn_head.conv.weight torch.Size([256, 256, 3, 3])\n",
      "proposal_generator.rpn_head.conv.bias torch.Size([256])\n",
      "proposal_generator.rpn_head.objectness_logits.weight torch.Size([3, 256, 1, 1])\n",
      "proposal_generator.rpn_head.objectness_logits.bias torch.Size([3])\n",
      "proposal_generator.rpn_head.anchor_deltas.weight torch.Size([12, 256, 1, 1])\n",
      "proposal_generator.rpn_head.anchor_deltas.bias torch.Size([12])\n",
      "roi_heads.box_head.fc1.weight torch.Size([1024, 12544])\n",
      "roi_heads.box_head.fc1.bias torch.Size([1024])\n",
      "roi_heads.box_head.fc2.weight torch.Size([1024, 1024])\n",
      "roi_heads.box_head.fc2.bias torch.Size([1024])\n",
      "roi_heads.box_predictor.bbox_pred.weight torch.Size([320, 1024])\n",
      "roi_heads.box_predictor.bbox_pred.bias torch.Size([320])\n",
      "roi_heads.mask_head.mask_fcn1.weight torch.Size([256, 256, 3, 3])\n",
      "roi_heads.mask_head.mask_fcn1.bias torch.Size([256])\n",
      "roi_heads.mask_head.mask_fcn2.weight torch.Size([256, 256, 3, 3])\n",
      "roi_heads.mask_head.mask_fcn2.bias torch.Size([256])\n",
      "roi_heads.mask_head.mask_fcn3.weight torch.Size([256, 256, 3, 3])\n",
      "roi_heads.mask_head.mask_fcn3.bias torch.Size([256])\n",
      "roi_heads.mask_head.mask_fcn4.weight torch.Size([256, 256, 3, 3])\n",
      "roi_heads.mask_head.mask_fcn4.bias torch.Size([256])\n",
      "roi_heads.mask_head.deconv.weight torch.Size([256, 256, 2, 2])\n",
      "roi_heads.mask_head.deconv.bias torch.Size([256])\n",
      "roi_heads.mask_head.predictor.weight torch.Size([80, 256, 1, 1])\n",
      "roi_heads.mask_head.predictor.bias torch.Size([80])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试结束",
   "id": "d9f6e1179b2cba89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "现在考虑将其与CLIP融合",
   "id": "8d3fd86ad8d13d0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MyZeroShotOpenVocabularyDetector(nn.Module):\n",
    "    def __init__(self, clip_path, my_mask_mrcnn_path, mrcnn_cfg, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_path).to(device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(clip_path)\n",
    "        \n",
    "        self.my_mask_rcnn = MyMaskRCNN(cfg=mrcnn_cfg)\n",
    "        self.my_mask_rcnn.load_from_pkl(my_mask_mrcnn_path)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def predict(self, dataset_dicts, meta_data=None, batch_size=1):\n",
    "        \"\"\"\n",
    "        dataset_dicts: list of dicts, Detectron2 dataset format\n",
    "        meta_data: MetadataCatalog.get(...) 对象，可选\n",
    "        batch_size: int\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        num_images = len(dataset_dicts)\n",
    "        \n",
    "        # 处理 batch\n",
    "        for i in range(0, num_images, batch_size):\n",
    "            batch = dataset_dicts[i:i+batch_size]\n",
    "            \n",
    "            # 将图片 tensor 移动到 device\n",
    "            images = [d[\"image\"].to(self.device) for d in batch]\n",
    "            # Detectron2 预处理（pad to max size, normalize）\n",
    "            images = self.my_mask_rcnn.preprocess_image(images)\n",
    "            \n",
    "            # backbone + rpn + roi_heads\n",
    "            features = self.my_mask_rcnn.backbone(images.tensor)\n",
    "            proposals, _ = self.my_mask_rcnn.proposal_generator(images, features)\n",
    "            \n",
    "            # ROI heads，得到 mask + box 修正\n",
    "            # 因为你需要 box_predictor 修正后的 box\n",
    "            detections, _ = self.my_mask_rcnn.roi_heads(images, features, proposals)\n",
    "            \n",
    "            # postprocess 每张图\n",
    "            for det, inp in zip(detections, batch):\n",
    "                # det 是 Instances 对象，包含:\n",
    "                # det.pred_boxes, det.pred_masks, det.scores, det.pred_classes\n",
    "                out_dict = {\n",
    "                    \"image_id\": inp[\"image_id\"],\n",
    "                    \"boxes\": det.pred_boxes.tensor.cpu(),    # 修正后的 boxes [N,4]\n",
    "                    \"scores\": det.scores.cpu(),\n",
    "                    \"classes\": det.pred_classes.cpu(),\n",
    "                    \"masks\": det.pred_masks.cpu()           # [N,H,W]\n",
    "                }\n",
    "                results.append(out_dict)\n",
    "        \n",
    "        return results\n",
    "    "
   ],
   "id": "41b6ee9c0dc8249e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
