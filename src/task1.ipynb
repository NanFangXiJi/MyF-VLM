{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "本方案直接对每个回归头输出的proposal裁剪出图像块，输入CLIP\n",
    "\n",
    "本方案不需要额外的训练"
   ],
   "id": "bcc7e58bce2c805d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:14:43.395711Z",
     "start_time": "2025-10-16T14:14:38.615761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.layers import cat\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import fast_rcnn_inference\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "import detectron2.data.transforms as T_\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import cv2\n",
    "from PIL import Image"
   ],
   "id": "e6bdf448647e1fa7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\SomeRandomTask\\CLIP-MRNN\\.venv\\lib\\site-packages\\detectron2\\model_zoo\\model_zoo.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "F:\\SomeRandomTask\\CLIP-MRNN\\.venv\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:14:43.411857Z",
     "start_time": "2025-10-16T14:14:43.395711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_mode = True\n",
    "\n",
    "val_json = \"../data/COCO/annotations/instances_val2017.json\"\n",
    "val_images = \"../data/COCO/val2017\"\n",
    "MRCNN_PATH = \"../model/model_final_f10217.pkl\"\n",
    "CONFIG_FILE = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "\n",
    "CLIP_PATH = \"../model/clip-vit-patch32/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268\"\n",
    "MyMRCNN_PATH = \"../model/my_mask_rcnn.pkl\"\n",
    "MyMRCNN_CLSFREE_PATH = \"../model/my_clsfree_mask_rcnn.pkl\""
   ],
   "id": "7939fc1ee7da7bcc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "首先读取Mask R-CNN，并将其分类头去掉",
   "id": "a3e84ead1cbc01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE))\n",
    "cfg.MODEL.WEIGHTS = MRCNN_PATH\n",
    "cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG = True\n",
    "#cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.DEVICE = \"cuda\""
   ],
   "id": "a95b8de601dcb79d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mask_rcnn_model = build_model(cfg)\n",
    "mask_rcnn_model.eval()\n",
    "DetectionCheckpointer(mask_rcnn_model).load(cfg.MODEL.WEIGHTS)"
   ],
   "id": "504b73d75253fd32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    for name, param in mask_rcnn_model.named_parameters():\n",
    "        print(name, param.shape)"
   ],
   "id": "4c43f72691740450",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "去掉：\n",
    "\n",
    "roi_heads.box_predictor.cls_score.weight torch.Size([81, 1024])\n",
    "\n",
    "roi_heads.box_predictor.cls_score.bias torch.Size([81])"
   ],
   "id": "6d419bb43a7cfe83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "构建MyMaskRCNN类来实现",
   "id": "55db1f8c00afbacf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:14:47.774957Z",
     "start_time": "2025-10-16T14:14:47.759325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyMaskRCNN(nn.Module):\n",
    "    def __init__(self, cfg=None, original_model=None):\n",
    "        \"\"\"\n",
    "        若提供 original_model：直接复制结构并去掉分类头；\n",
    "        若提供 cfg：根据配置文件构造同结构模型，再去掉分类头；\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if original_model is not None:\n",
    "            self.model = original_model\n",
    "        elif cfg is not None:\n",
    "            self.model = build_model(cfg)\n",
    "        else:\n",
    "            raise ValueError(\"必须提供 original_model 或 cfg 之一。\")\n",
    "\n",
    "        # 去掉不需要的分类头\n",
    "        if hasattr(self.model.roi_heads, \"box_predictor\") and hasattr(self.model.roi_heads.box_predictor, \"cls_score\"):\n",
    "            del self.model.roi_heads.box_predictor.cls_score\n",
    "            self.model.roi_heads.box_predictor.cls_score = None\n",
    "\n",
    "        # 应用原有的数据增强\n",
    "        self.aug = T_.ResizeShortestEdge(\n",
    "            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "        )\n",
    "\n",
    "    def save_to_pkl(self, path):\n",
    "        \"\"\"\n",
    "        将模型参数保存为.pkl文件\n",
    "        \"\"\"\n",
    "        state_dict = self.state_dict()\n",
    "        data = {\n",
    "            \"model\": state_dict,\n",
    "            \"__author__\": \"Task 1\",\n",
    "            \"matching_heuristics\": True\n",
    "        }\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def load_from_pkl(self, path):\n",
    "        \"\"\"\n",
    "        从.pkl文件加载模型参数\n",
    "        \"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        state_dict = data[\"model\"] if \"model\" in data else data\n",
    "        missing, unexpected = self.load_state_dict(state_dict, strict=False)\n",
    "        # print(f\"[MyMaskRCNN] 从 {path} 加载完成。\")\n",
    "        # if missing:\n",
    "        #     print(\"未加载参数：\", missing)\n",
    "        # if unexpected:\n",
    "        #     print(\"未使用参数：\", unexpected)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()"
   ],
   "id": "9861f5278994dbe0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CLS FREE\n",
    "my_mask_rcnn_model = MyMaskRCNN(original_model=mask_rcnn_model)\n",
    "my_mask_rcnn_model.eval()\n",
    "my_mask_rcnn_model.save_to_pkl(MyMRCNN_CLSFREE_PATH)"
   ],
   "id": "aceeeb98942a332c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "my_mask_rcnn_model = MyMaskRCNN(original_model=mask_rcnn_model)\n",
    "my_mask_rcnn_model.eval()\n",
    "my_mask_rcnn_model.save_to_pkl(MyMRCNN_PATH)"
   ],
   "id": "7995ceabc85e5faf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试",
   "id": "7fb798d0cd9e272b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    for name, param in my_mask_rcnn_model.named_parameters():\n",
    "        print(name, param.shape)"
   ],
   "id": "e18131c177dada5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    my_mask_rcnn_model = MyMaskRCNN(cfg=cfg)\n",
    "    my_mask_rcnn_model.load_from_pkl(MyMRCNN_PATH)\n",
    "    for name, param in my_mask_rcnn_model.named_parameters():\n",
    "        print(name, param.shape)"
   ],
   "id": "db2e9b2ea5b67f44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试结束",
   "id": "d9f6e1179b2cba89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "del my_mask_rcnn_model",
   "id": "db74df8439bdec3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "现在考虑将其与CLIP融合",
   "id": "8d3fd86ad8d13d0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CocoBatchDataset(Dataset):\n",
    "    def __init__(self, device=\"cpu\", dataset_name=\"coco_val\"):\n",
    "        self.dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "        self.meta_data = MetadataCatalog.get(dataset_name)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_dicts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.dataset_dicts[idx].copy()\n",
    "        # 读取 PIL 图像\n",
    "        d['pil'] = Image.open(d[\"file_name\"]).convert(\"RGB\")\n",
    "        # 读取 cv2 图像\n",
    "        org_img = cv2.imread(d[\"file_name\"])[:, :, ::-1]  # BGR -> RGB\n",
    "        d['cv2'] = org_img\n",
    "        # 转 tensor\n",
    "        img = org_img.astype(\"float32\").transpose(2, 0, 1)\n",
    "        d['image'] = torch.as_tensor(img).to(self.device)\n",
    "        return d\n",
    "\n",
    "def build_coco_batch_loader(dataset_name=\"coco_val\", mrcnn_batch_size=1, shuffle=False):\n",
    "    dataset = CocoBatchDataset(dataset_name)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=mrcnn_batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: batch  # 返回列表，每个元素是 dict\n",
    "    )"
   ],
   "id": "cab88227d8053f1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:14:52.787114Z",
     "start_time": "2025-10-16T14:14:52.755340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyZeroShotOpenVocabularyDetector(nn.Module):\n",
    "    def __init__(self, clip_path, my_mask_mrcnn_path, mrcnn_cfg, device, top_k=100, nms_thresh=0.5, score_thresh=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_path).to(device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(clip_path)\n",
    "\n",
    "        self.my_mask_rcnn = MyMaskRCNN(cfg=mrcnn_cfg)\n",
    "        self.my_mask_rcnn.load_from_pkl(my_mask_mrcnn_path)\n",
    "        self.my_mask_rcnn = self.my_mask_rcnn.to(device)\n",
    "        self.device = device\n",
    "\n",
    "        self.my_mask_rcnn.model.roi_heads.box_predictor.test_topk_per_image = top_k\n",
    "        self.my_mask_rcnn.model.roi_heads.box_predictor.test_nms_thresh = nms_thresh\n",
    "        self.my_mask_rcnn.model.roi_heads.box_predictor.test_score_thresh = score_thresh\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(self, batch, class_embeddings, clip_batch_size):\n",
    "\n",
    "        images_mrcnn = self.my_mask_rcnn.model.preprocess_image(batch)\n",
    "\n",
    "        features = self.my_mask_rcnn.model.backbone(images_mrcnn.tensor)\n",
    "        proposals, _ = self.my_mask_rcnn.model.proposal_generator(images_mrcnn, features)\n",
    "\n",
    "        orig_sizes = [(d[\"height\"], d[\"width\"]) for d in batch]\n",
    "        proc_sizes = [x.shape[-2:] for x in images_mrcnn.tensor]\n",
    "\n",
    "\n",
    "        boxes = [p.proposal_boxes.tensor for p in proposals]\n",
    "        cropped_images = self.images_crop([d[\"pil\"] for d in batch], boxes, orig_sizes, proc_sizes)\n",
    "\n",
    "        scores = self.clip_cls_pred(cropped_images, class_embeddings, clip_batch_size)\n",
    "        image_shapes = [x.image_size for x in proposals]\n",
    "\n",
    "        boxes = self.roi_heads_boxes(features, proposals)\n",
    "\n",
    "        pred_instances, _ = fast_rcnn_inference(\n",
    "            boxes,\n",
    "            tuple(scores.view(len(boxes), boxes[0].shape[0],-1)),\n",
    "            image_shapes,\n",
    "            self.my_mask_rcnn.model.roi_heads.box_predictor.test_score_thresh,\n",
    "            self.my_mask_rcnn.model.roi_heads.box_predictor.test_nms_thresh,\n",
    "            self.my_mask_rcnn.model.roi_heads.box_predictor.test_topk_per_image,\n",
    "        )\n",
    "\n",
    "        pred_instances = self.my_mask_rcnn.model.roi_heads.forward_with_given_boxes(features, pred_instances)\n",
    "        pred_instances = self.my_mask_rcnn.model._postprocess(pred_instances, batch, images_mrcnn.image_sizes)\n",
    "        return pred_instances\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, dataset_dicts, meta_data, mrcnn_batch_size=1, clip_batch_size=1, visualize=False,\n",
    "                visualize_path=None):\n",
    "        \n",
    "        loader = DataLoader(\n",
    "        CocoBatchDataset(dataset_dicts),\n",
    "        batch_size=mrcnn_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: batch  # 返回列表，每个元素是 dict\n",
    "        )\n",
    "        \n",
    "        results = []\n",
    "\n",
    "        class_name_list = meta_data.thing_classes\n",
    "        class_name_list = [\"a photo of \" + cls for cls in class_name_list]\n",
    "        class_name_list.append(\"background or no object\")\n",
    "        class_inputs = self.clip_processor(text=class_name_list, return_tensors=\"pt\", padding=True).to(\n",
    "            self.clip_model.device)\n",
    "        class_embeddings = self.clip_model.get_text_features(**class_inputs).to(self.clip_model.device)\n",
    "\n",
    "        for batch in loader:\n",
    "            pred_instances = self.forward(batch, class_embeddings, clip_batch_size)\n",
    "\n",
    "            # 后处理，按照detectron2的格式\n",
    "            for det, inp in zip(pred_instances, batch):\n",
    "                det[\"instances\"] = det[\"instances\"].to(\"cpu\")\n",
    "                out_dict = {\n",
    "                    \"image_id\": inp[\"image_id\"],\n",
    "                    \"instance\": det['instances'],\n",
    "                    \"image\": inp[\"image\"].cpu(),\n",
    "                    \"cv2\": inp[\"cv2\"],\n",
    "                    \"file_name\": inp[\"file_name\"]\n",
    "                }\n",
    "                results.append(out_dict)\n",
    "                \n",
    "            # 清理一下内存\n",
    "            del pred_instances, batch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if visualize:\n",
    "            self.visualize(results, visualize_path)\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def visualize(self, results, visualize_path):\n",
    "        for result in results:\n",
    "            img = result[\"cv2\"][:, :, ::-1]\n",
    "\n",
    "            # 创建 Visualizer\n",
    "            v = Visualizer(img, metadata=meta_data, scale=1.2)\n",
    "\n",
    "            out = v.draw_instance_predictions(result[\"instance\"])\n",
    "            img_vis = out.get_image()\n",
    "\n",
    "            img_path = os.path.join(visualize_path, f\"{os.path.basename(result['file_name'])}\")\n",
    "            if not cv2.imwrite(img_path, img_vis[..., ::-1]):  # 转回 BGR\n",
    "                raise IOError(f\"Failed to visualize image {img_path}\")\n",
    "            print(f\"saved at {img_path}\")\n",
    "\n",
    "    def roi_heads_boxes(self, features, proposals):\n",
    "        # 这一段改写自detectron2.modeling.roi_heads.roi_heads.StandardROIHeads._forward_box及其调用的函数\n",
    "        # 跳过了框的筛选等部分，这一部分会在CLIP预测类别后进行\n",
    "        features = [features[f] for f in self.my_mask_rcnn.model.roi_heads.box_in_features]\n",
    "        box_features = self.my_mask_rcnn.model.roi_heads.box_pooler(features, [x.proposal_boxes for x in proposals])\n",
    "        box_features = self.my_mask_rcnn.model.roi_heads.box_head(box_features)\n",
    "        if box_features.dim() > 2:\n",
    "            box_features = torch.flatten(box_features, start_dim=1)\n",
    "        proposal_deltas = self.my_mask_rcnn.model.roi_heads.box_predictor.bbox_pred(box_features)\n",
    "        del box_features\n",
    "\n",
    "        num_prop_per_image = [len(p) for p in proposals]\n",
    "        proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n",
    "        predict_boxes = self.my_mask_rcnn.model.roi_heads.box_predictor.box2box_transform.apply_deltas(\n",
    "            proposal_deltas,\n",
    "            proposal_boxes,\n",
    "        )  # Nx(KxB)\n",
    "        return predict_boxes.split(num_prop_per_image)\n",
    "\n",
    "    @staticmethod\n",
    "    def images_crop(images, boxes, orig_sizes, proc_sizes):\n",
    "        boxes_on_original = []\n",
    "        for boxes_per_image, (proc_h, proc_w), (orig_h, orig_w) in zip(boxes, proc_sizes, orig_sizes):\n",
    "            scale_x = orig_w / proc_w\n",
    "            scale_y = orig_h / proc_h\n",
    "            boxes_scaled = boxes_per_image.clone()\n",
    "            boxes_scaled[:, 0::2] *= scale_x\n",
    "            boxes_scaled[:, 1::2] *= scale_y\n",
    "            boxes_on_original.append(boxes_scaled)\n",
    "\n",
    "        cropped_images = []\n",
    "\n",
    "        for img, boxes_per_image in zip(images, boxes_on_original):\n",
    "            box_reshape = boxes_per_image.reshape(-1, 4)\n",
    "            for box in box_reshape:\n",
    "                x1, y1, x2, y2 = box.tolist()\n",
    "\n",
    "                # 计算宽高\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "\n",
    "                # 如果太小，则调整到最小尺寸\n",
    "                if w < 3:\n",
    "                    delta = (3 - w) / 2\n",
    "                    x1 = max(0, x1 - delta)\n",
    "                    x2 = min(img.width, x2 + delta)\n",
    "                if h < 3:\n",
    "                    delta = (3 - h) / 2\n",
    "                    y1 = max(0, y1 - delta)\n",
    "                    y2 = min(img.height, y2 + delta)\n",
    "\n",
    "                crop = img.crop((x1, y1, x2, y2))\n",
    "                cropped_images.append(np.array(crop).transpose(2, 0, 1))\n",
    "\n",
    "        return cropped_images\n",
    "\n",
    "    def clip_cls_pred(self, images, cls_embeddings, clip_batch_size):\n",
    "        img_embeddings_list = []\n",
    "\n",
    "        cnt = 0\n",
    "\n",
    "        for j in range(0, len(images), clip_batch_size):\n",
    "            if cnt % 5 == 0:\n",
    "                print(f\"the {cnt} * {clip_batch_size} = {cnt * clip_batch_size}\")\n",
    "            cnt += 1\n",
    "\n",
    "            batch_imgs = images[j:j + clip_batch_size]\n",
    "            inputs = self.clip_processor(images=batch_imgs, return_tensors='pt', padding=True).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                img_embeddings = self.clip_model.get_image_features(**inputs)\n",
    "                img_embeddings_list.append(img_embeddings)\n",
    "        img_embeddings = torch.cat(img_embeddings_list, dim=0)\n",
    "        similarity = img_embeddings @ cls_embeddings.T\n",
    "        probs = similarity.softmax(dim=-1)\n",
    "        return probs\n"
   ],
   "id": "41b6ee9c0dc8249e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "实例化",
   "id": "79528e17b3511406"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:14:56.439520Z",
     "start_time": "2025-10-16T14:14:54.122631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE))\n",
    "cfg.MODEL.WEIGHTS = MRCNN_PATH\n",
    "cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG = True\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "\n",
    "model = MyZeroShotOpenVocabularyDetector(CLIP_PATH, MyMRCNN_CLSFREE_PATH, cfg, torch.device(\"cuda\"), nms_thresh=0.3, score_thresh=0.5)\n",
    "model.eval();"
   ],
   "id": "9683b41408c85f72",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "COCO测试",
   "id": "ca5f9d88adec4750"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:14:57.601259Z",
     "start_time": "2025-10-16T14:14:56.439520Z"
    }
   },
   "cell_type": "code",
   "source": "register_coco_instances(\"coco_val\", {}, val_json, val_images)",
   "id": "f8947eecd209cb39",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "COCO小批量测试",
   "id": "4d948a4503bebf38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T14:16:00.496075Z",
     "start_time": "2025-10-16T14:14:57.601259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    meta_data = MetadataCatalog.get(\"coco_val\")\n",
    "    dataset_dicts = DatasetCatalog.get(\"coco_val\")\n",
    "    small_dataset_dicts = dataset_dicts[:4]\n",
    "    model.predict(small_dataset_dicts, meta_data, 4, 1000, visualize=True, visualize_path=\"../output/test\")\n",
    "    del meta_data, dataset_dicts, small_dataset_dicts"
   ],
   "id": "c5f11ccaa502aeae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\SomeRandomTask\\CLIP-MRNN\\.venv\\lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0 * 1000 = 0\n",
      "saved at ../output/test\\000000000139.jpg\n",
      "saved at ../output/test\\000000000285.jpg\n",
      "saved at ../output/test\\000000000632.jpg\n",
      "saved at ../output/test\\000000000724.jpg\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.predict(small_dataset_dicts, meta_data, 4, 1000, visualize=True, visualize_path=\"../output/task1\")",
   "id": "64a16a4f2c37f929",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "正式的测试模块",
   "id": "f594de892eb29835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader, DatasetCatalog, MetadataCatalog\n",
    "import torch\n",
    "\n",
    "\n",
    "def test(model, dataset_name=\"coco_val\", mrcnn_batch_size=1, clip_batch_size=1, visualize=False, visualize_path=None):\n",
    "    # 获取 COCO 数据和元信息\n",
    "    meta_data = MetadataCatalog.get(dataset_name)\n",
    "\n",
    "    # 初始化 COCO evaluator\n",
    "    evaluator = COCOEvaluator(dataset_name, output_dir=\"./coco_eval_output\")\n",
    "\n",
    "    # 构建测试数据加载器（仅供 evaluator 使用）\n",
    "    data_loader = build_detection_test_loader(dataset_name)\n",
    "\n",
    "    # 定义 wrapper，让 inference_on_dataset 能调用你的 predict\n",
    "    class ModelWrapper:\n",
    "        def __init__(self, model):\n",
    "            self.model = model\n",
    "\n",
    "        def __call__(self, dataset_dicts):\n",
    "            # 直接使用你写好的 predict()\n",
    "            return self.model.predict(\n",
    "                dataset_dicts,\n",
    "                meta_data=meta_data,\n",
    "                mrcnn_batch_size=mrcnn_batch_size,\n",
    "                clip_batch_size=clip_batch_size,\n",
    "                visualize=visualize,\n",
    "                visualize_path=visualize_path\n",
    "            )\n",
    "\n",
    "    print(f\"Starting evaluation on dataset: {dataset_name} ...\")\n",
    "\n",
    "    results = inference_on_dataset(ModelWrapper(model), data_loader, evaluator)\n",
    "\n",
    "    print(\"\\n===== Evaluation Results =====\")\n",
    "    for task, metrics in results.items():\n",
    "        print(f\"\\nTask: {task}\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"{k:30s}: {v:.4f}\")\n",
    "\n",
    "    return results\n"
   ],
   "id": "a8bbd7d8bbf3b6e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
