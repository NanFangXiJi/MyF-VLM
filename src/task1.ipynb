{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "本方案直接对每个回归头输出的proposal裁剪出图像块，输入CLIP\n",
    "\n",
    "本方案不需要额外的训练"
   ],
   "id": "bcc7e58bce2c805d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T08:50:01.180300Z",
     "start_time": "2025-10-19T08:49:56.158501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.layers import cat\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import fast_rcnn_inference\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "import detectron2.data.transforms as T_\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ],
   "id": "e6bdf448647e1fa7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\SomeRandomTask\\CLIP-MRNN\\.venv\\lib\\site-packages\\detectron2\\model_zoo\\model_zoo.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "F:\\SomeRandomTask\\CLIP-MRNN\\.venv\\lib\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T08:50:01.196141Z",
     "start_time": "2025-10-19T08:50:01.180846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_mode = True  # 测试代码正确性\n",
    "\n",
    "val_json = \"../data/COCO/annotations/instances_val2017.json\"\n",
    "val_images = \"../data/COCO/val2017\"\n",
    "\n",
    "val_small_json = \"./val_small.json\"\n",
    "\n",
    "MRCNN_PATH = \"../model/model_final_f10217.pkl\"\n",
    "CONFIG_FILE = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "\n",
    "CLIP_PATH = \"../model/clip-vit-patch32/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268\"\n",
    "MyMRCNN_PATH = \"../model/my_mask_rcnn.pkl\"\n",
    "MyMRCNN_CLSFREE_PATH = \"../model/my_clsfree_mask_rcnn.pkl\""
   ],
   "id": "7939fc1ee7da7bcc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "首先读取Mask R-CNN，并将其分类头去掉",
   "id": "a3e84ead1cbc01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T08:50:01.227453Z",
     "start_time": "2025-10-19T08:50:01.196141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE))\n",
    "cfg.MODEL.WEIGHTS = MRCNN_PATH\n",
    "cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG = True\n",
    "#cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.DEVICE = \"cuda\""
   ],
   "id": "a95b8de601dcb79d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T08:50:02.486969Z",
     "start_time": "2025-10-19T08:50:01.227453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask_rcnn_model = build_model(cfg)\n",
    "mask_rcnn_model.eval()\n",
    "DetectionCheckpointer(mask_rcnn_model).load(cfg.MODEL.WEIGHTS)"
   ],
   "id": "504b73d75253fd32",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001B[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'__author__': 'Detectron2 Model Zoo'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    for name, param in mask_rcnn_model.named_parameters():\n",
    "        print(name, param.shape)"
   ],
   "id": "4c43f72691740450",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "去掉：\n",
    "\n",
    "roi_heads.box_predictor.cls_score.weight torch.Size([81, 1024])\n",
    "\n",
    "roi_heads.box_predictor.cls_score.bias torch.Size([81])"
   ],
   "id": "6d419bb43a7cfe83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "构建MyMaskRCNN类来实现",
   "id": "55db1f8c00afbacf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T08:50:02.502203Z",
     "start_time": "2025-10-19T08:50:02.486969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyMaskRCNN(nn.Module):\n",
    "    def __init__(self, cfg=None, original_model=None):\n",
    "        \"\"\"\n",
    "        若提供 original_model：直接复制结构并去掉分类头；\n",
    "        若提供 cfg：根据配置文件构造同结构模型，再去掉分类头；\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if original_model is not None:\n",
    "            self.model = original_model\n",
    "        elif cfg is not None:\n",
    "            self.model = build_model(cfg)\n",
    "        else:\n",
    "            raise ValueError(\"必须提供 original_model 或 cfg 之一。\")\n",
    "\n",
    "        # 去掉不需要的分类头\n",
    "        if hasattr(self.model.roi_heads, \"box_predictor\") and hasattr(self.model.roi_heads.box_predictor, \"cls_score\"):\n",
    "            del self.model.roi_heads.box_predictor.cls_score\n",
    "            self.model.roi_heads.box_predictor.cls_score = None\n",
    "\n",
    "        # 应用原有的数据增强\n",
    "        self.aug = T_.ResizeShortestEdge(\n",
    "            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "        )\n",
    "\n",
    "    def save_to_pkl(self, path):\n",
    "        \"\"\"\n",
    "        将模型参数保存为.pkl文件\n",
    "        \"\"\"\n",
    "        state_dict = self.state_dict()\n",
    "        data = {\n",
    "            \"model\": state_dict,\n",
    "            \"__author__\": \"Task 1\",\n",
    "            \"matching_heuristics\": True\n",
    "        }\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def load_from_pkl(self, path):\n",
    "        \"\"\"\n",
    "        从.pkl文件加载模型参数\n",
    "        \"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        state_dict = data[\"model\"] if \"model\" in data else data\n",
    "        missing, unexpected = self.load_state_dict(state_dict, strict=False)\n",
    "        # print(f\"[MyMaskRCNN] 从 {path} 加载完成。\")\n",
    "        # if missing:\n",
    "        #     print(\"未加载参数：\", missing)\n",
    "        # if unexpected:\n",
    "        #     print(\"未使用参数：\", unexpected)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()"
   ],
   "id": "9861f5278994dbe0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CLS FREE\n",
    "my_mask_rcnn_model = MyMaskRCNN(original_model=mask_rcnn_model)\n",
    "my_mask_rcnn_model.eval()\n",
    "my_mask_rcnn_model.save_to_pkl(MyMRCNN_CLSFREE_PATH)"
   ],
   "id": "aceeeb98942a332c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "my_mask_rcnn_model = MyMaskRCNN(original_model=mask_rcnn_model)\n",
    "my_mask_rcnn_model.eval()\n",
    "my_mask_rcnn_model.save_to_pkl(MyMRCNN_PATH)"
   ],
   "id": "7995ceabc85e5faf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试",
   "id": "7fb798d0cd9e272b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    for name, param in my_mask_rcnn_model.named_parameters():\n",
    "        print(name, param.shape)"
   ],
   "id": "e18131c177dada5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    my_mask_rcnn_model = MyMaskRCNN(cfg=cfg)\n",
    "    my_mask_rcnn_model.load_from_pkl(MyMRCNN_PATH)\n",
    "    for name, param in my_mask_rcnn_model.named_parameters():\n",
    "        print(name, param.shape)"
   ],
   "id": "db2e9b2ea5b67f44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试结束",
   "id": "d9f6e1179b2cba89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "del my_mask_rcnn_model",
   "id": "db74df8439bdec3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "现在考虑将其与CLIP融合",
   "id": "8d3fd86ad8d13d0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T08:50:04.850692Z",
     "start_time": "2025-10-19T08:50:04.830265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, aug, dataset_name=None, dataset_dicts=None, meta_data=None):\n",
    "        if dataset_name is None:\n",
    "            if dataset_dicts is None or meta_data is None:\n",
    "                raise ValueError(\"dataset_name为None时dataset_dicts与meta_data不能为None\")\n",
    "            else:\n",
    "                self.dataset_dicts = dataset_dicts\n",
    "                self.meta_data = meta_data\n",
    "        else:\n",
    "            self.dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "            self.meta_data = MetadataCatalog.get(dataset_name)\n",
    "            \n",
    "        self.aug = aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_dicts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.dataset_dicts[idx].copy()\n",
    "        # 读取 cv2 图像\n",
    "        org_img = cv2.imread(d[\"file_name\"])\n",
    "        d['cv2'] = org_img\n",
    "        # 这一部分来自defaults的__call__\n",
    "        # 转 tensor\n",
    "        img = self.aug.get_transform(org_img).apply_image(org_img)\n",
    "        d['image'] = torch.as_tensor(img.astype(\"float32\").transpose(2, 0, 1))\n",
    "        return d\n",
    "\n",
    "def build_batch_loader(aug, dataset_name=None, dataset_dicts=None, meta_data=None, mrcnn_batch_size=1, shuffle=False):\n",
    "    if dataset_name is None:\n",
    "        if dataset_dicts is None or meta_data is None:\n",
    "            raise ValueError(\"dataset_name为None时dataset_dicts与meta_data不能为None\")\n",
    "        else:\n",
    "            dataset = MyDataset(aug, dataset_dicts=dataset_dicts, meta_data=meta_data)\n",
    "    else:\n",
    "        dataset = MyDataset(aug, dataset_name=dataset_name)\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=mrcnn_batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: batch\n",
    "    )"
   ],
   "id": "cab88227d8053f1c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T08:50:06.059041Z",
     "start_time": "2025-10-19T08:50:06.029197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyZeroShotOpenVocabularyDetector(nn.Module):\n",
    "    def __init__(self, clip_path, my_mask_mrcnn_path, mrcnn_cfg, device, top_k=100, nms_thresh=0.5, score_thresh=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_path).to(device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(clip_path)\n",
    "\n",
    "        self.my_mask_rcnn = MyMaskRCNN(cfg=mrcnn_cfg)\n",
    "        self.my_mask_rcnn.load_from_pkl(my_mask_mrcnn_path)\n",
    "        self.my_mask_rcnn = self.my_mask_rcnn.to(device)\n",
    "        self.device = device\n",
    "\n",
    "        self.my_mask_rcnn.model.roi_heads.box_predictor.test_topk_per_image = top_k\n",
    "        self.my_mask_rcnn.model.roi_heads.box_predictor.test_nms_thresh = nms_thresh\n",
    "        self.my_mask_rcnn.model.roi_heads.box_predictor.test_score_thresh = score_thresh\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(self, batch, class_embeddings, clip_batch_size):\n",
    "\n",
    "        images_mrcnn = self.my_mask_rcnn.model.preprocess_image(batch)\n",
    "\n",
    "        features = self.my_mask_rcnn.model.backbone(images_mrcnn.tensor)\n",
    "        proposals, _ = self.my_mask_rcnn.model.proposal_generator(images_mrcnn, features)\n",
    "\n",
    "        orig_sizes = [(d[\"height\"], d[\"width\"]) for d in batch]\n",
    "        proc_sizes = [x.shape[-2:] for x in images_mrcnn.tensor]\n",
    "\n",
    "\n",
    "        boxes = [p.proposal_boxes.tensor for p in proposals]\n",
    "        cropped_images = self.images_crop([d[\"cv2\"] for d in batch], boxes, orig_sizes, proc_sizes)\n",
    "\n",
    "        scores = self.clip_cls_pred(cropped_images, class_embeddings, clip_batch_size)\n",
    "        image_shapes = [x.image_size for x in proposals]\n",
    "\n",
    "        boxes = self.roi_heads_boxes(features, proposals)\n",
    "\n",
    "        pred_instances, _ = fast_rcnn_inference(\n",
    "            boxes,\n",
    "            tuple(scores.view(len(boxes), boxes[0].shape[0],-1)),\n",
    "            image_shapes,\n",
    "            self.my_mask_rcnn.model.roi_heads.box_predictor.test_score_thresh,\n",
    "            self.my_mask_rcnn.model.roi_heads.box_predictor.test_nms_thresh,\n",
    "            self.my_mask_rcnn.model.roi_heads.box_predictor.test_topk_per_image,\n",
    "        )\n",
    "\n",
    "        pred_instances = self.my_mask_rcnn.model.roi_heads.forward_with_given_boxes(features, pred_instances)\n",
    "        pred_instances = self.my_mask_rcnn.model._postprocess(pred_instances, batch, images_mrcnn.image_sizes)\n",
    "        return pred_instances\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, dataset_name, mrcnn_batch_size=1, clip_batch_size=1, visualize=False,\n",
    "                visualize_path=None):\n",
    "        \n",
    "        dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "        meta_data = MetadataCatalog.get(dataset_name)\n",
    "        \n",
    "        loader = build_batch_loader(self.my_mask_rcnn.aug, dataset_dicts=dataset_dicts, meta_data=meta_data,\n",
    "                                    mrcnn_batch_size=mrcnn_batch_size, shuffle=False)\n",
    "        \n",
    "        results = []\n",
    "\n",
    "        class_name_list = self.class_name_list_prepare(meta_data.thing_classes)\n",
    "        class_embeddings = self.get_cls_embedding(class_name_list)\n",
    "\n",
    "        for batch in loader:\n",
    "            pred_instances = self.forward(batch, class_embeddings, clip_batch_size)\n",
    "\n",
    "            # 后处理，按照detectron2的格式\n",
    "            for det, inp in zip(pred_instances, batch):\n",
    "                det[\"instances\"] = det[\"instances\"].to(\"cpu\")\n",
    "                out_dict = {\n",
    "                    \"image_id\": inp[\"image_id\"],\n",
    "                    \"instance\": det['instances'],\n",
    "                    \"image\": inp[\"image\"].cpu(),\n",
    "                    \"cv2\": inp[\"cv2\"],\n",
    "                    \"file_name\": inp[\"file_name\"]\n",
    "                }\n",
    "                results.append(out_dict)\n",
    "                \n",
    "            # 清理一下内存\n",
    "            del pred_instances, batch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if visualize:\n",
    "            self.visualize(results, visualize_path, meta_data)\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def visualize(self, results, visualize_path, meta_data):\n",
    "        for result in results:\n",
    "            img = result[\"cv2\"][:, :, ::-1]\n",
    "\n",
    "            # 创建 Visualizer\n",
    "            v = Visualizer(img, metadata=meta_data, scale=1.2)\n",
    "\n",
    "            out = v.draw_instance_predictions(result[\"instance\"])\n",
    "            img_vis = out.get_image()\n",
    "\n",
    "            img_path = os.path.join(visualize_path, f\"{os.path.basename(result['file_name'])}\")\n",
    "            if not cv2.imwrite(img_path, img_vis[..., ::-1]):  # 转回 BGR\n",
    "                raise IOError(f\"Failed to visualize image {img_path}\")\n",
    "            print(f\"saved at {img_path}\")\n",
    "\n",
    "    def roi_heads_boxes(self, features, proposals):\n",
    "        # 这一段改写自detectron2.modeling.roi_heads.roi_heads.StandardROIHeads._forward_box及其调用的函数\n",
    "        # 跳过了框的筛选等部分，这一部分会在CLIP预测类别后进行\n",
    "        features = [features[f] for f in self.my_mask_rcnn.model.roi_heads.box_in_features]\n",
    "        box_features = self.my_mask_rcnn.model.roi_heads.box_pooler(features, [x.proposal_boxes for x in proposals])\n",
    "        box_features = self.my_mask_rcnn.model.roi_heads.box_head(box_features)\n",
    "        if box_features.dim() > 2:\n",
    "            box_features = torch.flatten(box_features, start_dim=1)\n",
    "        proposal_deltas = self.my_mask_rcnn.model.roi_heads.box_predictor.bbox_pred(box_features)\n",
    "        del box_features\n",
    "\n",
    "        num_prop_per_image = [len(p) for p in proposals]\n",
    "        proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n",
    "        predict_boxes = self.my_mask_rcnn.model.roi_heads.box_predictor.box2box_transform.apply_deltas(\n",
    "            proposal_deltas,\n",
    "            proposal_boxes,\n",
    "        )  # Nx(KxB)\n",
    "        return predict_boxes.split(num_prop_per_image)\n",
    "    \n",
    "    @staticmethod\n",
    "    def images_crop(images, boxes, orig_sizes, proc_sizes):\n",
    "        boxes_on_original = []\n",
    "        for boxes_per_image, (proc_h, proc_w), (orig_h, orig_w) in zip(boxes, proc_sizes, orig_sizes):\n",
    "            scale_x = orig_w / proc_w\n",
    "            scale_y = orig_h / proc_h\n",
    "            boxes_scaled = boxes_per_image.clone()\n",
    "            boxes_scaled[:, 0::2] *= scale_x\n",
    "            boxes_scaled[:, 1::2] *= scale_y\n",
    "            boxes_on_original.append(boxes_scaled)\n",
    "\n",
    "        cropped_images = []\n",
    "\n",
    "        for img, boxes_per_image in zip(images, boxes_on_original):\n",
    "            box_reshape = boxes_per_image.reshape(-1, 4)\n",
    "            h_img, w_img = img.shape[:2]\n",
    "\n",
    "            for box in box_reshape:\n",
    "                x1, y1, x2, y2 = box.tolist()\n",
    "\n",
    "                # 计算宽高\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "\n",
    "                # 如果太小，则调整到最小尺寸\n",
    "                if w < 3:\n",
    "                    delta = (3 - w) / 2\n",
    "                    x1 = max(0, x1 - delta)\n",
    "                    x2 = min(w_img, x2 + delta)\n",
    "                if h < 3:\n",
    "                    delta = (3 - h) / 2\n",
    "                    y1 = max(0, y1 - delta)\n",
    "                    y2 = min(h_img, y2 + delta)\n",
    "\n",
    "                # 转为 int 并截断边界\n",
    "                x1 = int(np.clip(x1, 0, w_img - 1))\n",
    "                x2 = int(np.clip(x2, 0, w_img))\n",
    "                y1 = int(np.clip(y1, 0, h_img - 1))\n",
    "                y2 = int(np.clip(y2, 0, h_img))\n",
    "\n",
    "                # OpenCV 切片裁剪 ([y1:y2, x1:x2])\n",
    "                crop = img[y1:y2, x1:x2]\n",
    "\n",
    "                # 转换为 CHW 格式\n",
    "                cropped_images.append(np.transpose(crop, (2, 0, 1)))\n",
    "\n",
    "        return cropped_images\n",
    "\n",
    "    def clip_cls_pred(self, images, cls_embeddings, clip_batch_size):\n",
    "        img_embeddings_list = []\n",
    "\n",
    "        cnt = 0\n",
    "\n",
    "        for j in range(0, len(images), clip_batch_size):\n",
    "            if cnt % 5 == 0:\n",
    "                print(f\"the {cnt} * {clip_batch_size} = {cnt * clip_batch_size}\")\n",
    "            cnt += 1\n",
    "\n",
    "            batch_imgs = images[j:j + clip_batch_size]\n",
    "            inputs = self.clip_processor(images=batch_imgs, return_tensors='pt', padding=True).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                img_embeddings = self.clip_model.get_image_features(**inputs)\n",
    "                img_embeddings_list.append(img_embeddings)\n",
    "        img_embeddings = torch.cat(img_embeddings_list, dim=0)\n",
    "        similarity = img_embeddings @ cls_embeddings.T\n",
    "        probs = similarity.softmax(dim=-1)\n",
    "        return probs\n",
    "    \n",
    "    def class_name_list_prepare(self, class_name_list):\n",
    "        #class_name_list = [\"a photo of \" + cls for cls in class_name_list]  # TODO: zero-shot 测试？\n",
    "        class_name_list.append(\"background or no object\")  # TODO: 确认background怎么弄\n",
    "        return class_name_list\n",
    "        \n",
    "    def get_cls_embedding(self, class_name_list):\n",
    "        class_inputs = self.clip_processor(text=class_name_list, return_tensors=\"pt\", padding=True).to(\n",
    "            self.clip_model.device)\n",
    "        return self.clip_model.get_text_features(**class_inputs).to(self.clip_model.device)"
   ],
   "id": "41b6ee9c0dc8249e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "实例化",
   "id": "79528e17b3511406"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T08:50:09.420028Z",
     "start_time": "2025-10-19T08:50:07.306186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE))\n",
    "cfg.MODEL.WEIGHTS = MRCNN_PATH\n",
    "cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG = True\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "\n",
    "model = MyZeroShotOpenVocabularyDetector(CLIP_PATH, MyMRCNN_CLSFREE_PATH, cfg, torch.device(\"cuda\"), nms_thresh=0.2, score_thresh=0.2)\n",
    "model.eval();"
   ],
   "id": "9683b41408c85f72",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "注册完整COCO",
   "id": "ca5f9d88adec4750"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "register_coco_instances(\"coco_val\", {}, val_json, val_images)",
   "id": "f8947eecd209cb39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "生成小批量用于测试代码并保存",
   "id": "ae7d33dbb3de75ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(val_json, \"r\") as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "small_images = coco_data[\"images\"][:5]\n",
    "small_image_ids = {img[\"id\"] for img in small_images}\n",
    "\n",
    "# 过滤出对应的标注\n",
    "small_annotations = [ann for ann in coco_data[\"annotations\"] if ann[\"image_id\"] in small_image_ids]\n",
    "\n",
    "# 构建新的 COCO JSON\n",
    "small_coco = {\n",
    "    \"info\": coco_data.get(\"info\", {}),\n",
    "    \"licenses\": coco_data.get(\"licenses\", []),\n",
    "    \"images\": small_images,\n",
    "    \"annotations\": small_annotations,\n",
    "    \"categories\": coco_data[\"categories\"]\n",
    "}\n",
    "\n",
    "# 保存到临时 JSON 文件\n",
    "with open(val_small_json, \"w\") as f:\n",
    "    json.dump(small_coco, f)"
   ],
   "id": "5b26b479c15fa591",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "注册小批量COCO",
   "id": "4adcfbffc4a2851e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T08:50:14.252977Z",
     "start_time": "2025-10-19T08:50:14.236644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    register_coco_instances(\"coco_val_small\", {}, val_small_json, val_images)"
   ],
   "id": "8390fee8cac4a68d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "COCO小批量测试",
   "id": "4d948a4503bebf38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    model.predict(\"coco_val_small\", 5, 1000, visualize=True, visualize_path=\"../output/test\")"
   ],
   "id": "c5f11ccaa502aeae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "正式的测试模块",
   "id": "f594de892eb29835"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T08:56:07.148971Z",
     "start_time": "2025-10-19T08:50:32.936835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model: MyZeroShotOpenVocabularyDetector, dataset_name=None, output_dir=None, mrcnn_batch_size=1, clip_batch_size=1):\n",
    "    dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "    meta_data = MetadataCatalog.get(dataset_name)\n",
    "\n",
    "    loader = build_batch_loader(model.my_mask_rcnn.aug, dataset_dicts=dataset_dicts, meta_data=meta_data,\n",
    "                                mrcnn_batch_size=mrcnn_batch_size, shuffle=False)\n",
    "\n",
    "    evaluator = COCOEvaluator(dataset_name if dataset_name else \"custom_coco\", output_dir=output_dir)\n",
    "    evaluator.reset()\n",
    "    \n",
    "    class_name_list = model.class_name_list_prepare(meta_data.thing_classes)\n",
    "    class_embeddings = model.get_cls_embedding(class_name_list)\n",
    "\n",
    "    for batch in loader:\n",
    "        with torch.no_grad():\n",
    "            pred_instances = model.forward(batch, class_embeddings=class_embeddings, clip_batch_size=clip_batch_size)\n",
    "\n",
    "        # 转换为 Detectron2 标准格式\n",
    "        for det, inp in zip(pred_instances, batch):\n",
    "            det[\"instances\"] = det[\"instances\"].to(\"cpu\")\n",
    "            out_dict = {\n",
    "                \"image_id\": inp[\"image_id\"],\n",
    "                \"instances\": det[\"instances\"],\n",
    "                \"height\": inp[\"height\"],\n",
    "                \"width\": inp[\"width\"]\n",
    "            }\n",
    "            \n",
    "            evaluator.process([inp], [out_dict])\n",
    "\n",
    "    metrics = evaluator.evaluate()\n",
    "\n",
    "    return metrics\n",
    "test(model, \"coco_val_small\", output_dir=\"../output/test2\", mrcnn_batch_size=2, clip_batch_size=1000)"
   ],
   "id": "a8bbd7d8bbf3b6e6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\SomeRandomTask\\CLIP-MRNN\\.venv\\lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0 * 1000 = 0\n",
      "the 0 * 1000 = 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 33\u001B[0m\n\u001B[0;32m     30\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate()\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m metrics\n\u001B[1;32m---> 33\u001B[0m \u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcoco_val_small\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../output/test2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmrcnn_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclip_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[10], line 28\u001B[0m, in \u001B[0;36mtest\u001B[1;34m(model, dataset_name, output_dir, mrcnn_batch_size, clip_batch_size)\u001B[0m\n\u001B[0;32m     20\u001B[0m         det[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstances\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m det[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstances\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     21\u001B[0m         out_dict \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     22\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: inp[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage_id\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     23\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstances\u001B[39m\u001B[38;5;124m\"\u001B[39m: det[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstances\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     24\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m\"\u001B[39m: inp[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     25\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m\"\u001B[39m: inp[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     26\u001B[0m         }\n\u001B[1;32m---> 28\u001B[0m         \u001B[43mevaluator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minp\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mout_dict\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m metrics \u001B[38;5;241m=\u001B[39m evaluator\u001B[38;5;241m.\u001B[39mevaluate()\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m metrics\n",
      "File \u001B[1;32mF:\\SomeRandomTask\\CLIP-MRNN\\.venv\\lib\\site-packages\\detectron2\\evaluation\\coco_evaluation.py:159\u001B[0m, in \u001B[0;36mCOCOEvaluator.process\u001B[1;34m(self, inputs, outputs)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstances\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m output:\n\u001B[0;32m    158\u001B[0m     instances \u001B[38;5;241m=\u001B[39m output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstances\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cpu_device)\n\u001B[1;32m--> 159\u001B[0m     prediction[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstances\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43minstances_to_coco_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43minstances\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimage_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproposals\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m output:\n\u001B[0;32m    161\u001B[0m     prediction[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproposals\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproposals\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cpu_device)\n",
      "File \u001B[1;32mF:\\SomeRandomTask\\CLIP-MRNN\\.venv\\lib\\site-packages\\detectron2\\evaluation\\coco_evaluation.py:409\u001B[0m, in \u001B[0;36minstances_to_coco_json\u001B[1;34m(instances, img_id)\u001B[0m\n\u001B[0;32m    402\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_mask:\n\u001B[0;32m    403\u001B[0m     \u001B[38;5;66;03m# use RLE to encode the masks, because they are too large and takes memory\u001B[39;00m\n\u001B[0;32m    404\u001B[0m     \u001B[38;5;66;03m# since this evaluator stores outputs of the entire dataset\u001B[39;00m\n\u001B[0;32m    405\u001B[0m     rles \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    406\u001B[0m         mask_util\u001B[38;5;241m.\u001B[39mencode(np\u001B[38;5;241m.\u001B[39marray(mask[:, :, \u001B[38;5;28;01mNone\u001B[39;00m], order\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mF\u001B[39m\u001B[38;5;124m\"\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muint8\u001B[39m\u001B[38;5;124m\"\u001B[39m))[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    407\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m mask \u001B[38;5;129;01min\u001B[39;00m instances\u001B[38;5;241m.\u001B[39mpred_masks\n\u001B[0;32m    408\u001B[0m     ]\n\u001B[1;32m--> 409\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m rle \u001B[38;5;129;01min\u001B[39;00m \u001B[43mrles\u001B[49m:\n\u001B[0;32m    410\u001B[0m         \u001B[38;5;66;03m# \"counts\" is an array encoded by mask_util as a byte-stream. Python3's\u001B[39;00m\n\u001B[0;32m    411\u001B[0m         \u001B[38;5;66;03m# json writer which always produces strings cannot serialize a bytestream\u001B[39;00m\n\u001B[0;32m    412\u001B[0m         \u001B[38;5;66;03m# unless you decode it. Thankfully, utf-8 works out (which is also what\u001B[39;00m\n\u001B[0;32m    413\u001B[0m         \u001B[38;5;66;03m# the pycocotools/_mask.pyx does).\u001B[39;00m\n\u001B[0;32m    414\u001B[0m         rle[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcounts\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m rle[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcounts\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    416\u001B[0m has_keypoints \u001B[38;5;241m=\u001B[39m instances\u001B[38;5;241m.\u001B[39mhas(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpred_keypoints\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mF:\\SomeRandomTask\\CLIP-MRNN\\.venv\\lib\\site-packages\\detectron2\\evaluation\\coco_evaluation.py:409\u001B[0m, in \u001B[0;36minstances_to_coco_json\u001B[1;34m(instances, img_id)\u001B[0m\n\u001B[0;32m    402\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_mask:\n\u001B[0;32m    403\u001B[0m     \u001B[38;5;66;03m# use RLE to encode the masks, because they are too large and takes memory\u001B[39;00m\n\u001B[0;32m    404\u001B[0m     \u001B[38;5;66;03m# since this evaluator stores outputs of the entire dataset\u001B[39;00m\n\u001B[0;32m    405\u001B[0m     rles \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    406\u001B[0m         mask_util\u001B[38;5;241m.\u001B[39mencode(np\u001B[38;5;241m.\u001B[39marray(mask[:, :, \u001B[38;5;28;01mNone\u001B[39;00m], order\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mF\u001B[39m\u001B[38;5;124m\"\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muint8\u001B[39m\u001B[38;5;124m\"\u001B[39m))[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    407\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m mask \u001B[38;5;129;01min\u001B[39;00m instances\u001B[38;5;241m.\u001B[39mpred_masks\n\u001B[0;32m    408\u001B[0m     ]\n\u001B[1;32m--> 409\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m rle \u001B[38;5;129;01min\u001B[39;00m \u001B[43mrles\u001B[49m:\n\u001B[0;32m    410\u001B[0m         \u001B[38;5;66;03m# \"counts\" is an array encoded by mask_util as a byte-stream. Python3's\u001B[39;00m\n\u001B[0;32m    411\u001B[0m         \u001B[38;5;66;03m# json writer which always produces strings cannot serialize a bytestream\u001B[39;00m\n\u001B[0;32m    412\u001B[0m         \u001B[38;5;66;03m# unless you decode it. Thankfully, utf-8 works out (which is also what\u001B[39;00m\n\u001B[0;32m    413\u001B[0m         \u001B[38;5;66;03m# the pycocotools/_mask.pyx does).\u001B[39;00m\n\u001B[0;32m    414\u001B[0m         rle[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcounts\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m rle[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcounts\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    416\u001B[0m has_keypoints \u001B[38;5;241m=\u001B[39m instances\u001B[38;5;241m.\u001B[39mhas(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpred_keypoints\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1103\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1065\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\Tool\\PyCharm 2024.1.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1201\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1198\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1200\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1201\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Tool\\PyCharm 2024.1.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1216\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1213\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1215\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1216\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1218\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1220\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    test(model, \"coco_val_small\", output_dir=\"../output/test2\", mrcnn_batch_size=5, clip_batch_size=1000)"
   ],
   "id": "b0b0f9c62fa504cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f29d6147374e9c06",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
