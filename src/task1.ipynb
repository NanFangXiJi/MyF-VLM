{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "本方案直接对每个回归头输出的proposal裁剪出图像块，输入CLIP\n",
    "\n",
    "本方案不需要额外的训练"
   ],
   "id": "bcc7e58bce2c805d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.layers import cat\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import fast_rcnn_inference\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "import detectron2.data.transforms as T_\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import cv2\n",
    "from PIL import Image"
   ],
   "id": "e6bdf448647e1fa7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_mode = True  # 测试代码正确性\n",
    "\n",
    "val_json = \"../data/COCO/annotations/instances_val2017.json\"\n",
    "val_images = \"../data/COCO/val2017\"\n",
    "\n",
    "val_small_json = \"./val_small.json\"\n",
    "\n",
    "MRCNN_PATH = \"../model/model_final_f10217.pkl\"\n",
    "CONFIG_FILE = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "\n",
    "CLIP_PATH = \"../model/clip-vit-patch32/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268\"\n",
    "MyMRCNN_PATH = \"../model/my_mask_rcnn.pkl\"\n",
    "MyMRCNN_CLSFREE_PATH = \"../model/my_clsfree_mask_rcnn.pkl\""
   ],
   "id": "7939fc1ee7da7bcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "首先读取Mask R-CNN，并将其分类头去掉",
   "id": "a3e84ead1cbc01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE))\n",
    "cfg.MODEL.WEIGHTS = MRCNN_PATH\n",
    "cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG = True\n",
    "#cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.DEVICE = \"cuda\""
   ],
   "id": "a95b8de601dcb79d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mask_rcnn_model = build_model(cfg)\n",
    "mask_rcnn_model.eval()\n",
    "DetectionCheckpointer(mask_rcnn_model).load(cfg.MODEL.WEIGHTS)"
   ],
   "id": "504b73d75253fd32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    for name, param in mask_rcnn_model.named_parameters():\n",
    "        print(name, param.shape)"
   ],
   "id": "4c43f72691740450",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "去掉：\n",
    "\n",
    "roi_heads.box_predictor.cls_score.weight torch.Size([81, 1024])\n",
    "\n",
    "roi_heads.box_predictor.cls_score.bias torch.Size([81])"
   ],
   "id": "6d419bb43a7cfe83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "构建MyMaskRCNN类来实现",
   "id": "55db1f8c00afbacf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MyMaskRCNN(nn.Module):\n",
    "    def __init__(self, cfg=None, original_model=None):\n",
    "        \"\"\"\n",
    "        若提供 original_model：直接复制结构并去掉分类头；\n",
    "        若提供 cfg：根据配置文件构造同结构模型，再去掉分类头；\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if original_model is not None:\n",
    "            self.model = original_model\n",
    "        elif cfg is not None:\n",
    "            self.model = build_model(cfg)\n",
    "        else:\n",
    "            raise ValueError(\"必须提供 original_model 或 cfg 之一。\")\n",
    "\n",
    "        # 去掉不需要的分类头\n",
    "        if hasattr(self.model.roi_heads, \"box_predictor\") and hasattr(self.model.roi_heads.box_predictor, \"cls_score\"):\n",
    "            del self.model.roi_heads.box_predictor.cls_score\n",
    "            self.model.roi_heads.box_predictor.cls_score = None\n",
    "\n",
    "        # 应用原有的数据增强\n",
    "        self.aug = T_.ResizeShortestEdge(\n",
    "            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "        )\n",
    "\n",
    "    def save_to_pkl(self, path):\n",
    "        \"\"\"\n",
    "        将模型参数保存为.pkl文件\n",
    "        \"\"\"\n",
    "        state_dict = self.state_dict()\n",
    "        data = {\n",
    "            \"model\": state_dict,\n",
    "            \"__author__\": \"Task 1\",\n",
    "            \"matching_heuristics\": True\n",
    "        }\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    def load_from_pkl(self, path):\n",
    "        \"\"\"\n",
    "        从.pkl文件加载模型参数\n",
    "        \"\"\"\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        state_dict = data[\"model\"] if \"model\" in data else data\n",
    "        missing, unexpected = self.load_state_dict(state_dict, strict=False)\n",
    "        # print(f\"[MyMaskRCNN] 从 {path} 加载完成。\")\n",
    "        # if missing:\n",
    "        #     print(\"未加载参数：\", missing)\n",
    "        # if unexpected:\n",
    "        #     print(\"未使用参数：\", unexpected)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()"
   ],
   "id": "9861f5278994dbe0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CLS FREE\n",
    "my_mask_rcnn_model = MyMaskRCNN(original_model=mask_rcnn_model)\n",
    "my_mask_rcnn_model.eval()\n",
    "my_mask_rcnn_model.save_to_pkl(MyMRCNN_CLSFREE_PATH)"
   ],
   "id": "aceeeb98942a332c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "my_mask_rcnn_model = MyMaskRCNN(original_model=mask_rcnn_model)\n",
    "my_mask_rcnn_model.eval()\n",
    "my_mask_rcnn_model.save_to_pkl(MyMRCNN_PATH)"
   ],
   "id": "7995ceabc85e5faf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试",
   "id": "7fb798d0cd9e272b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    for name, param in my_mask_rcnn_model.named_parameters():\n",
    "        print(name, param.shape)"
   ],
   "id": "e18131c177dada5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    my_mask_rcnn_model = MyMaskRCNN(cfg=cfg)\n",
    "    my_mask_rcnn_model.load_from_pkl(MyMRCNN_PATH)\n",
    "    for name, param in my_mask_rcnn_model.named_parameters():\n",
    "        print(name, param.shape)"
   ],
   "id": "db2e9b2ea5b67f44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试结束",
   "id": "d9f6e1179b2cba89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "del my_mask_rcnn_model",
   "id": "db74df8439bdec3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "现在考虑将其与CLIP融合",
   "id": "8d3fd86ad8d13d0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CocoBatchDataset(Dataset):\n",
    "    def __init__(self, aug, dataset_name=None, dataset_dicts=None, meta_data=None):\n",
    "        if dataset_name is None:\n",
    "            if dataset_dicts is None or meta_data is None:\n",
    "                raise ValueError(\"dataset_name为None时dataset_dicts与meta_data不能为None\")\n",
    "            else:\n",
    "                self.dataset_dicts = dataset_dicts\n",
    "                self.meta_data = meta_data\n",
    "        else:\n",
    "            self.dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "            self.meta_data = MetadataCatalog.get(dataset_name)\n",
    "            \n",
    "        self.aug = aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_dicts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.dataset_dicts[idx].copy()\n",
    "        # 读取 PIL 图像\n",
    "        d['pil'] = Image.open(d[\"file_name\"])\n",
    "        # 读取 cv2 图像\n",
    "        org_img = cv2.imread(d[\"file_name\"])\n",
    "        d['cv2'] = org_img\n",
    "        # 这一部分来自defaults的__call__\n",
    "        # 转 tensor\n",
    "        img = self.aug.get_transform(org_img).apply_image(org_img)\n",
    "        d['image'] = torch.as_tensor(img.astype(\"float32\").transpose(2, 0, 1))\n",
    "        return d\n",
    "\n",
    "def build_batch_loader(aug, dataset_name=None, dataset_dicts=None, meta_data=None, mrcnn_batch_size=1, shuffle=False):\n",
    "    if dataset_name is None:\n",
    "        if dataset_dicts is None or meta_data is None:\n",
    "            raise ValueError(\"dataset_name为None时dataset_dicts与meta_data不能为None\")\n",
    "        else:\n",
    "            dataset = CocoBatchDataset(aug, dataset_dicts=dataset_dicts, meta_data=meta_data)\n",
    "    else:\n",
    "        dataset = CocoBatchDataset(aug, dataset_name=dataset_name)\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=mrcnn_batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: batch\n",
    "    )"
   ],
   "id": "cab88227d8053f1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MyZeroShotOpenVocabularyDetector(nn.Module):\n",
    "    def __init__(self, clip_path, my_mask_mrcnn_path, mrcnn_cfg, device, top_k=100, nms_thresh=0.5, score_thresh=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_path).to(device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(clip_path)\n",
    "\n",
    "        self.my_mask_rcnn = MyMaskRCNN(cfg=mrcnn_cfg)\n",
    "        self.my_mask_rcnn.load_from_pkl(my_mask_mrcnn_path)\n",
    "        self.my_mask_rcnn = self.my_mask_rcnn.to(device)\n",
    "        self.device = device\n",
    "\n",
    "        self.my_mask_rcnn.model.roi_heads.box_predictor.test_topk_per_image = top_k\n",
    "        self.my_mask_rcnn.model.roi_heads.box_predictor.test_nms_thresh = nms_thresh\n",
    "        self.my_mask_rcnn.model.roi_heads.box_predictor.test_score_thresh = score_thresh\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(self, batch, class_embeddings, clip_batch_size):\n",
    "\n",
    "        images_mrcnn = self.my_mask_rcnn.model.preprocess_image(batch)\n",
    "\n",
    "        features = self.my_mask_rcnn.model.backbone(images_mrcnn.tensor)\n",
    "        proposals, _ = self.my_mask_rcnn.model.proposal_generator(images_mrcnn, features)\n",
    "\n",
    "        orig_sizes = [(d[\"height\"], d[\"width\"]) for d in batch]\n",
    "        proc_sizes = [x.shape[-2:] for x in images_mrcnn.tensor]\n",
    "\n",
    "\n",
    "        boxes = [p.proposal_boxes.tensor for p in proposals]\n",
    "        cropped_images = self.images_crop([d[\"pil\"] for d in batch], boxes, orig_sizes, proc_sizes)\n",
    "\n",
    "        scores = self.clip_cls_pred(cropped_images, class_embeddings, clip_batch_size)\n",
    "        image_shapes = [x.image_size for x in proposals]\n",
    "\n",
    "        boxes = self.roi_heads_boxes(features, proposals)\n",
    "\n",
    "        pred_instances, _ = fast_rcnn_inference(\n",
    "            boxes,\n",
    "            tuple(scores.view(len(boxes), boxes[0].shape[0],-1)),\n",
    "            image_shapes,\n",
    "            self.my_mask_rcnn.model.roi_heads.box_predictor.test_score_thresh,\n",
    "            self.my_mask_rcnn.model.roi_heads.box_predictor.test_nms_thresh,\n",
    "            self.my_mask_rcnn.model.roi_heads.box_predictor.test_topk_per_image,\n",
    "        )\n",
    "\n",
    "        pred_instances = self.my_mask_rcnn.model.roi_heads.forward_with_given_boxes(features, pred_instances)\n",
    "        pred_instances = self.my_mask_rcnn.model._postprocess(pred_instances, batch, images_mrcnn.image_sizes)\n",
    "        return pred_instances\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, dataset_name, mrcnn_batch_size=1, clip_batch_size=1, visualize=False,\n",
    "                visualize_path=None):\n",
    "        \n",
    "        dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "        meta_data = MetadataCatalog.get(dataset_name)\n",
    "        \n",
    "        loader = build_batch_loader(self.my_mask_rcnn.aug, dataset_dicts=dataset_dicts, meta_data=meta_data,\n",
    "                                    mrcnn_batch_size=mrcnn_batch_size, shuffle=False)\n",
    "        \n",
    "        results = []\n",
    "\n",
    "        class_name_list = self.class_name_list_prepare(meta_data.thing_classes)\n",
    "        class_embeddings = self.get_cls_embedding(class_name_list)\n",
    "\n",
    "        for batch in loader:\n",
    "            pred_instances = self.forward(batch, class_embeddings, clip_batch_size)\n",
    "\n",
    "            # 后处理，按照detectron2的格式\n",
    "            for det, inp in zip(pred_instances, batch):\n",
    "                det[\"instances\"] = det[\"instances\"].to(\"cpu\")\n",
    "                out_dict = {\n",
    "                    \"image_id\": inp[\"image_id\"],\n",
    "                    \"instance\": det['instances'],\n",
    "                    \"image\": inp[\"image\"].cpu(),\n",
    "                    \"cv2\": inp[\"cv2\"],\n",
    "                    \"file_name\": inp[\"file_name\"]\n",
    "                }\n",
    "                results.append(out_dict)\n",
    "                \n",
    "            # 清理一下内存\n",
    "            del pred_instances, batch\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if visualize:\n",
    "            self.visualize(results, visualize_path, meta_data)\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def visualize(self, results, visualize_path, meta_data):\n",
    "        for result in results:\n",
    "            img = result[\"cv2\"][:, :, ::-1]\n",
    "\n",
    "            # 创建 Visualizer\n",
    "            v = Visualizer(img, metadata=meta_data, scale=1.2)\n",
    "\n",
    "            out = v.draw_instance_predictions(result[\"instance\"])\n",
    "            img_vis = out.get_image()\n",
    "\n",
    "            img_path = os.path.join(visualize_path, f\"{os.path.basename(result['file_name'])}\")\n",
    "            if not cv2.imwrite(img_path, img_vis[..., ::-1]):  # 转回 BGR\n",
    "                raise IOError(f\"Failed to visualize image {img_path}\")\n",
    "            print(f\"saved at {img_path}\")\n",
    "\n",
    "    def roi_heads_boxes(self, features, proposals):\n",
    "        # 这一段改写自detectron2.modeling.roi_heads.roi_heads.StandardROIHeads._forward_box及其调用的函数\n",
    "        # 跳过了框的筛选等部分，这一部分会在CLIP预测类别后进行\n",
    "        features = [features[f] for f in self.my_mask_rcnn.model.roi_heads.box_in_features]\n",
    "        box_features = self.my_mask_rcnn.model.roi_heads.box_pooler(features, [x.proposal_boxes for x in proposals])\n",
    "        box_features = self.my_mask_rcnn.model.roi_heads.box_head(box_features)\n",
    "        if box_features.dim() > 2:\n",
    "            box_features = torch.flatten(box_features, start_dim=1)\n",
    "        proposal_deltas = self.my_mask_rcnn.model.roi_heads.box_predictor.bbox_pred(box_features)\n",
    "        del box_features\n",
    "\n",
    "        num_prop_per_image = [len(p) for p in proposals]\n",
    "        proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n",
    "        predict_boxes = self.my_mask_rcnn.model.roi_heads.box_predictor.box2box_transform.apply_deltas(\n",
    "            proposal_deltas,\n",
    "            proposal_boxes,\n",
    "        )  # Nx(KxB)\n",
    "        return predict_boxes.split(num_prop_per_image)\n",
    "\n",
    "    @staticmethod\n",
    "    def images_crop(images, boxes, orig_sizes, proc_sizes):\n",
    "        boxes_on_original = []\n",
    "        for boxes_per_image, (proc_h, proc_w), (orig_h, orig_w) in zip(boxes, proc_sizes, orig_sizes):\n",
    "            scale_x = orig_w / proc_w\n",
    "            scale_y = orig_h / proc_h\n",
    "            boxes_scaled = boxes_per_image.clone()\n",
    "            boxes_scaled[:, 0::2] *= scale_x\n",
    "            boxes_scaled[:, 1::2] *= scale_y\n",
    "            boxes_on_original.append(boxes_scaled)\n",
    "\n",
    "        cropped_images = []\n",
    "\n",
    "        for img, boxes_per_image in zip(images, boxes_on_original):\n",
    "            box_reshape = boxes_per_image.reshape(-1, 4)\n",
    "            for box in box_reshape:\n",
    "                x1, y1, x2, y2 = box.tolist()\n",
    "\n",
    "                # 计算宽高\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "\n",
    "                # 如果太小，则调整到最小尺寸\n",
    "                if w < 3:\n",
    "                    delta = (3 - w) / 2\n",
    "                    x1 = max(0, x1 - delta)\n",
    "                    x2 = min(img.width, x2 + delta)\n",
    "                if h < 3:\n",
    "                    delta = (3 - h) / 2\n",
    "                    y1 = max(0, y1 - delta)\n",
    "                    y2 = min(img.height, y2 + delta)\n",
    "\n",
    "                crop = img.crop((x1, y1, x2, y2))\n",
    "                cropped_images.append(np.array(crop).transpose(2, 0, 1))\n",
    "\n",
    "        return cropped_images\n",
    "\n",
    "    def clip_cls_pred(self, images, cls_embeddings, clip_batch_size):\n",
    "        img_embeddings_list = []\n",
    "\n",
    "        cnt = 0\n",
    "\n",
    "        for j in range(0, len(images), clip_batch_size):\n",
    "            if cnt % 5 == 0:\n",
    "                print(f\"the {cnt} * {clip_batch_size} = {cnt * clip_batch_size}\")\n",
    "            cnt += 1\n",
    "\n",
    "            batch_imgs = images[j:j + clip_batch_size]\n",
    "            inputs = self.clip_processor(images=batch_imgs, return_tensors='pt', padding=True).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                img_embeddings = self.clip_model.get_image_features(**inputs)\n",
    "                img_embeddings_list.append(img_embeddings)\n",
    "        img_embeddings = torch.cat(img_embeddings_list, dim=0)\n",
    "        similarity = img_embeddings @ cls_embeddings.T\n",
    "        probs = similarity.softmax(dim=-1)\n",
    "        return probs\n",
    "    \n",
    "    def class_name_list_prepare(self, class_name_list):\n",
    "        #class_name_list = [\"a photo of \" + cls for cls in class_name_list]  # TODO: zero-shot 测试\n",
    "        class_name_list.append(\"background or no object\")  # TODO: 确认background怎么弄\n",
    "        return class_name_list\n",
    "        \n",
    "    def get_cls_embedding(self, class_name_list):\n",
    "        class_inputs = self.clip_processor(text=class_name_list, return_tensors=\"pt\", padding=True).to(\n",
    "            self.clip_model.device)\n",
    "        return self.clip_model.get_text_features(**class_inputs).to(self.clip_model.device)"
   ],
   "id": "41b6ee9c0dc8249e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "实例化",
   "id": "79528e17b3511406"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE))\n",
    "cfg.MODEL.WEIGHTS = MRCNN_PATH\n",
    "cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG = True\n",
    "cfg.MODEL.DEVICE = \"cuda\"\n",
    "\n",
    "model = MyZeroShotOpenVocabularyDetector(CLIP_PATH, MyMRCNN_CLSFREE_PATH, cfg, torch.device(\"cuda\"), score_thresh=0.5, nms_thresh=0.3)\n",
    "model.eval();"
   ],
   "id": "9683b41408c85f72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "COCO测试",
   "id": "ca5f9d88adec4750"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "register_coco_instances(\"coco_val\", {}, val_json, val_images)",
   "id": "f8947eecd209cb39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "生成小批量用于测试代码",
   "id": "ae7d33dbb3de75ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 读取原始 COCO JSON\n",
    "with open(val_json, \"r\") as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# 取前 4 张图片\n",
    "small_images = coco_data[\"images\"][:5]\n",
    "small_image_ids = {img[\"id\"] for img in small_images}\n",
    "\n",
    "# 过滤出对应的标注\n",
    "small_annotations = [ann for ann in coco_data[\"annotations\"] if ann[\"image_id\"] in small_image_ids]\n",
    "\n",
    "# 构建新的 COCO JSON\n",
    "small_coco = {\n",
    "    \"info\": coco_data.get(\"info\", {}),            # 拷贝原始 info\n",
    "    \"licenses\": coco_data.get(\"licenses\", []),    # 拷贝原始 licenses\n",
    "    \"images\": small_images,\n",
    "    \"annotations\": small_annotations,\n",
    "    \"categories\": coco_data[\"categories\"]\n",
    "}\n",
    "\n",
    "# 保存到临时 JSON 文件\n",
    "with open(val_small_json, \"w\") as f:\n",
    "    json.dump(small_coco, f)"
   ],
   "id": "5b26b479c15fa591",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    register_coco_instances(\"coco_val_small\", {}, val_small_json, val_images)"
   ],
   "id": "8390fee8cac4a68d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "COCO小批量测试",
   "id": "4d948a4503bebf38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    model.predict(\"coco_val_small\", 5, 1000, visualize=True, visualize_path=\"../output/test\")"
   ],
   "id": "c5f11ccaa502aeae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "正式的测试模块",
   "id": "f594de892eb29835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test(model: MyZeroShotOpenVocabularyDetector, dataset_name=None, output_dir=None, mrcnn_batch_size=1, clip_batch_size=1):\n",
    "    dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "    meta_data = MetadataCatalog.get(dataset_name)\n",
    "\n",
    "    loader = build_batch_loader(model.my_mask_rcnn.aug, dataset_dicts=dataset_dicts, meta_data=meta_data,\n",
    "                                mrcnn_batch_size=mrcnn_batch_size, shuffle=False)\n",
    "\n",
    "    evaluator = COCOEvaluator(dataset_name if dataset_name else \"custom_coco\", output_dir=output_dir)\n",
    "    evaluator.reset()\n",
    "    \n",
    "    class_name_list = model.class_name_list_prepare(meta_data.thing_classes)\n",
    "    class_embeddings = model.get_cls_embedding(class_name_list)\n",
    "\n",
    "    for batch in loader:\n",
    "        with torch.no_grad():\n",
    "            pred_instances = model.forward(batch, class_embeddings=class_embeddings, clip_batch_size=clip_batch_size)\n",
    "\n",
    "        # 转换为 Detectron2 标准格式\n",
    "        for det, inp in zip(pred_instances, batch):\n",
    "            det[\"instances\"] = det[\"instances\"].to(\"cpu\")\n",
    "            out_dict = {\n",
    "                \"image_id\": inp[\"image_id\"],\n",
    "                \"instances\": det[\"instances\"],\n",
    "                \"height\": inp[\"height\"],\n",
    "                \"width\": inp[\"width\"]\n",
    "            }\n",
    "            \n",
    "            evaluator.process([inp], [out_dict])\n",
    "\n",
    "    metrics = evaluator.evaluate()\n",
    "\n",
    "    return metrics\n"
   ],
   "id": "a8bbd7d8bbf3b6e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if test_mode:\n",
    "    test(model, \"coco_val_small\", output_dir=\"../output/test2\", mrcnn_batch_size=5, clip_batch_size=1000)"
   ],
   "id": "b0b0f9c62fa504cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f29d6147374e9c06",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
