{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "本方案参考F-VLM设计\n",
    "\n",
    "需要微调"
   ],
   "id": "45413b9dde2d0031"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionTransformer\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from detectron2.modeling.meta_arch.rcnn import GeneralizedRCNN\n",
    "from detectron2.modeling.proposal_generator.rpn import RPN\n",
    "from detectron2.modeling.roi_heads.roi_heads import StandardROIHeads\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.structures import Boxes, Instances\n",
    "from detectron2.data.datasets import register_coco_instances"
   ],
   "id": "6a8af5b19cd3784f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val_json = \"../data/COCO/annotations/instances_val2017.json\"\n",
    "val_images = \"../data/COCO/val2017\"\n",
    "\n",
    "train_json = \"../data/COCO/annotations/instances_train2017.json\"\n",
    "train_images = \"../data/COCO/train2017\"\n",
    "\n",
    "val_small_json = \"./val_small.json\"\n",
    "\n",
    "MRCNN_PATH = \"../model/model_final_f10217.pkl\"\n",
    "CONFIG_FILE = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "\n",
    "CLIP_PATH = \"../model/clip-vit-patch32/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268\"\n",
    "MyMRCNN_PATH = \"../model/my_mask_rcnn.pkl\"\n",
    "MyMRCNN_CLSFREE_PATH = \"../model/my_clsfree_mask_rcnn.pkl\""
   ],
   "id": "f54109a1cced2b37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "实现余弦退火调度器",
   "id": "6e100cef10da73a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cosine_scheduler_with_warmup(optimizer, warmup_steps, total_steps, min_lr_ratio=0.1):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        progress = (current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return min_lr_ratio + (1 - min_lr_ratio) * cosine_decay\n",
    "    return LambdaLR(optimizer, lr_lambda)"
   ],
   "id": "110daadabaf1d85f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "这里有一部分元件需要从头训练，因此给出两种初始化代码备用",
   "id": "f546be0ce9296283"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def init_weights_kaiming(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "def init_weights_xavier(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ],
   "id": "e5fb97405543867e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "这里还存在对比损失，这里也给出 InfoNCE 对比损失的实现",
   "id": "4508c8322ce0a17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def contrastive_loss(scores: torch.Tensor, gt_classes: torch.Tensor, temperature: float = 0.07):\n",
    "    # softmax 温度缩放\n",
    "    logits = scores / temperature  \n",
    "\n",
    "    # 使用交叉熵作为对比损失\n",
    "    loss = F.cross_entropy(logits, gt_classes)\n",
    "    return loss"
   ],
   "id": "d94b9d8e19f3d680"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "按照 F-VLM 的设计，CLIP可以分为两部分，分别是特征提取与最后的Pooling层。\n",
    "\n",
    "这里将其分出来"
   ],
   "id": "e98567a453a8bcee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CLIPVisionTransformerSplit(CLIPVisionTransformer):\n",
    "    def forward_features(self, pixel_values):\n",
    "        \"\"\"对应 feature extractor 部分\"\"\"\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "        hidden_states = self.pre_layrnorm(hidden_states)\n",
    "        encoder_outputs = self.encoder(inputs_embeds=hidden_states)\n",
    "        return encoder_outputs[0]  # last_hidden_state\n",
    "\n",
    "    def forward_pool(self, last_hidden_state):\n",
    "        \"\"\"对应 last feature pooling layer 部分，这一部分的输出不能直接用，还需要 visual projection 来投影\"\"\"\n",
    "        pooled_output = last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.post_layernorm(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TwoStageCLIPModel(CLIPModel):\n",
    "    \"\"\"\n",
    "    拓展版 CLIP 模型，支持显式分离视觉编码的两个阶段：\n",
    "    1. Feature Extractor (patch embedding + transformer encoder)\n",
    "    2. Last Feature Pooling Layer (CLS pooling + LayerNorm)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.vision_model = CLIPVisionTransformerSplit(config.vision_config)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        model = super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
    "\n",
    "        vision_config = model.config.vision_config\n",
    "        new_vision_model = CLIPVisionTransformerSplit(vision_config)\n",
    "        new_vision_model.load_state_dict(model.vision_model.state_dict())\n",
    "        model.vision_model = new_vision_model\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_image_features_stage1(self, pixel_values: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        获取图像的 patch-level 特征 (Feature Extractor 输出)\n",
    "        对应 self.vision_model.embeddings + self.vision_model.encoder\n",
    "        这里的输出中，seq_len维度的首位是CLS，其余是图像各分块的embeddings。\n",
    "        \"\"\"\n",
    "        return self.vision_model.forward_features(pixel_values)  # shape: (batch, seq_len, hidden_dim)\n",
    "\n",
    "    def get_image_features_stage2(self, last_hidden_state: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        从 stage1 输出计算 pooled image feature\n",
    "        对应 self.vision_model.post_layernorm(CLS token) + self.visual_projection(pooled_state)\n",
    "        \"\"\"\n",
    "        pooled_state = self.vision_model.forward_pool(last_hidden_state)  # shape: (batch, hidden_dim)\n",
    "        return self.visual_projection(pooled_state)"
   ],
   "id": "14d6ae44a95fe26b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "原本的 RPN 设计的 forward 需要传入的东西拿不出来，这里新建子类重写之",
   "id": "507f3c3345e5c18e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MyRPN(RPN):\n",
    "    def forward(self, image_sizes, features, gt_instances):\n",
    "        \"\"\"\n",
    "        原本传入 image 只为了 image_size，这次直接传入image_size，避免还要image\n",
    "        \"\"\"\n",
    "        features = [features[f] for f in self.in_features]\n",
    "        anchors = self.anchor_generator(features)\n",
    "\n",
    "        pred_objectness_logits, pred_anchor_deltas = self.rpn_head(features)\n",
    "        # Transpose the Hi*Wi*A dimension to the middle:\n",
    "        pred_objectness_logits = [\n",
    "            # (N, A, Hi, Wi) -> (N, Hi, Wi, A) -> (N, Hi*Wi*A)\n",
    "            score.permute(0, 2, 3, 1).flatten(1)\n",
    "            for score in pred_objectness_logits\n",
    "        ]\n",
    "        pred_anchor_deltas = [\n",
    "            # (N, A*B, Hi, Wi) -> (N, A, B, Hi, Wi) -> (N, Hi, Wi, A, B) -> (N, Hi*Wi*A, B)\n",
    "            x.view(x.shape[0], -1, self.anchor_generator.box_dim, x.shape[-2], x.shape[-1])\n",
    "            .permute(0, 3, 4, 1, 2)\n",
    "            .flatten(1, -2)\n",
    "            for x in pred_anchor_deltas\n",
    "        ]\n",
    "\n",
    "        if self.training:\n",
    "            assert gt_instances is not None, \"RPN requires gt_instances in training!\"\n",
    "            gt_labels, gt_boxes = self.label_and_sample_anchors(anchors, gt_instances)\n",
    "            losses = self.losses(\n",
    "                anchors, pred_objectness_logits, gt_labels, pred_anchor_deltas, gt_boxes\n",
    "            )\n",
    "        else:\n",
    "            losses = {}\n",
    "        proposals = self.predict_proposals(\n",
    "            anchors, pred_objectness_logits, pred_anchor_deltas, image_sizes\n",
    "        )\n",
    "        return proposals, losses"
   ],
   "id": "e38b7b63dc4140e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Box Predictor需要重写 forward 与 loss\n",
    "\n",
    "loss 的类别应该改为对比损失"
   ],
   "id": "228093038caeb3b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MyBoxPredictor(FastRCNNOutputLayers):\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: per-region features of shape (N, ...) for N bounding boxes to predict.\n",
    "\n",
    "        Returns:\n",
    "            (Tensor, Tensor):\n",
    "            First tensor: shape (N,K+1), scores for each of the N box. Each row contains the\n",
    "            scores for K object categories and 1 background class.\n",
    "\n",
    "            Second tensor: bounding box regression deltas for each box. Shape is shape (N,Kx4),\n",
    "            or (N,4) for class-agnostic regression.\n",
    "        \"\"\"\n",
    "        if x.dim() > 2:\n",
    "            x = torch.flatten(x, start_dim=1)\n",
    "        # scores = self.cls_score(x)  # scores不需要有\n",
    "        proposal_deltas = self.bbox_pred(x)\n",
    "        return None, proposal_deltas\n",
    "    \n",
    "    def losses(self, predictions, proposals):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were used\n",
    "                to compute predictions. The fields ``proposal_boxes``, ``gt_boxes``,\n",
    "                ``gt_classes`` are expected.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Tensor]: dict of losses\n",
    "        \"\"\"\n",
    "        scores, proposal_deltas = predictions\n",
    "\n",
    "        # parse classification outputs\n",
    "        gt_classes = torch.cat([p.gt_classes for p in proposals], dim=0)\n",
    "\n",
    "        # parse box regression outputs\n",
    "        proposal_boxes = torch.cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n",
    "        assert not proposal_boxes.requires_grad, \"Proposals should not require gradients!\"\n",
    "        # If \"gt_boxes\" does not exist, the proposals must be all negative and\n",
    "        # should not be included in regression loss computation.\n",
    "        # Here we just use proposal_boxes as an arbitrary placeholder because its\n",
    "        # value won't be used in self.box_reg_loss().\n",
    "        gt_boxes = torch.cat(\n",
    "            [(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        losses = {\n",
    "            \"loss_cls\": contrastive_loss(scores, gt_classes),\n",
    "            \"loss_box_reg\": self.box_reg_loss(\n",
    "                proposal_boxes, gt_boxes, proposal_deltas, gt_classes\n",
    "            ),\n",
    "        }\n",
    "        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}"
   ],
   "id": "663d7a67e9bca82e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "重写 ROI Head 的逻辑",
   "id": "abc19b0f66414e85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MyROIHeads(StandardROIHeads):\n",
    "    def __init__(self, *, box_in_features, box_pooler, box_head, box_predictor: nn.Module, **kwargs):\n",
    "        super().__init__(box_in_features=box_in_features, box_pooler=box_pooler, box_head=box_head,\n",
    "                         box_predictor=box_predictor, **kwargs)\n",
    "        self.projection = nn.Linear(1024, 512)  # 用于对齐的层\n",
    "\n",
    "    def forward(self, features, proposals, text_embeddings, targets=None):\n",
    "        if self.training:\n",
    "            assert targets, \"'targets' argument is required during training\"\n",
    "            proposals = self.label_and_sample_proposals(proposals, targets)\n",
    "        del targets\n",
    "\n",
    "        if self.training:\n",
    "            losses = self._forward_box(features, proposals)\n",
    "            # Usually the original proposals used by the box head are used by the mask, keypoint\n",
    "            # heads. But when `self.train_on_pred_boxes is True`, proposals will contain boxes\n",
    "            # predicted by the box head.\n",
    "            losses.update(self._forward_mask(features, proposals))\n",
    "            return proposals, losses\n",
    "        else:\n",
    "            pred_instances = self._forward_box(features, proposals, text_embeddings)\n",
    "            # During inference cascaded prediction is used: the mask and keypoints heads are only\n",
    "            # applied to the top scoring box detections.\n",
    "            pred_instances = self.forward_with_given_boxes(features, pred_instances)\n",
    "            return pred_instances, {}\n",
    "        \n",
    "    def _forward_box(self, features, proposals, text_embeddings):\n",
    "        features = [features[f] for f in self.box_in_features]\n",
    "        box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])\n",
    "        box_features = self.box_head(box_features)\n",
    "        _, box_deltas = self.box_predictor(box_features)\n",
    "        box_features = self.projection(box_features)\n",
    "        box_features = F.normalize(box_features, p=2, dim=1)  # 这就是最终的 embedding 了\n",
    "        \n",
    "        scores = box_features @ text_embeddings.T\n",
    "\n",
    "        if self.training:\n",
    "            losses = self.box_predictor.losses((scores, box_deltas), proposals)\n",
    "            # proposals is modified in-place below, so losses must be computed first.\n",
    "            if self.train_on_pred_boxes:\n",
    "                with torch.no_grad():\n",
    "                    pred_boxes = self.box_predictor.predict_boxes_for_gt_classes(\n",
    "                        (scores, box_deltas), proposals\n",
    "                    )\n",
    "                    for proposals_per_image, pred_boxes_per_image in zip(proposals, pred_boxes):\n",
    "                        proposals_per_image.proposal_boxes = Boxes(pred_boxes_per_image)\n",
    "            return losses\n",
    "        else:\n",
    "            pred_instances, _ = self.box_predictor.inference((scores, box_deltas), proposals)\n",
    "            return pred_instances"
   ],
   "id": "580d3cad68adadb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "对 Mask R-CNN 的实现",
   "id": "eb7d370c2eea5f4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MyMRCNN2(GeneralizedRCNN):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "        \n",
    "    def forward(self, image_sizes, clip_feature, gt_instances, text_embeddings):\n",
    "        if not self.training:\n",
    "            return self.inference(clip_feature, gt_instances)\n",
    "        \n",
    "        features = self.backbone(clip_feature)\n",
    "        proposals, proposal_losses = self.proposal_generator(image_sizes, features, gt_instances)  # 这里会产生初始框proposal的损失\n",
    "        _, detector_losses = self.roi_heads(features, proposals, text_embeddings, gt_instances)\n",
    "        \n",
    "        \"\"\"可视化不在这里进行了\n",
    "        if self.vis_period > 0:\n",
    "            storage = get_event_storage()\n",
    "            if storage.iter % self.vis_period == 0:\n",
    "                self.visualize_training(batched_inputs, proposals)\n",
    "        \"\"\"\n",
    "\n",
    "        losses = {}\n",
    "        losses.update(detector_losses)\n",
    "        losses.update(proposal_losses)\n",
    "        return losses\n",
    "        \n",
    "    def inference(self, clip_feature, gt_instances):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, cfg_path, weight_path, device=\"cuda\"):\n",
    "        cfg = get_cfg()\n",
    "        cfg.merge_from_file(cfg_path)\n",
    "        cfg.MODEL.WEIGHTS = weight_path\n",
    "        cfg.MODEL.DEVICE = device\n",
    "\n",
    "        model = cls(cfg)\n",
    "        checkpointer = DetectionCheckpointer(model)\n",
    "        checkpointer.load(weight_path)\n",
    "        \n",
    "        # fpn 的 bottom_up 改为 CLIP 专属的 Adapter\n",
    "        model.backbone.bottom_up = CLIPtoFPNAdapter()\n",
    "        \n",
    "        # 各类型改为更便利的子类\n",
    "        model.proposal_generator.__class__ = MyRPN\n",
    "        \n",
    "        model.roi_heads.__class__ = MyROIHeads\n",
    "        model.roi_heads.projection = nn.Linear(1024, 512)  # 还需要添加一个投影来对齐类别特征与文本embeddings\n",
    "        \n",
    "        model.roi_heads.box_predictor.__class__ = MyBoxPredictor\n",
    "        model.roi_heads.box_predictor.cls_score = None  # 移除不需要的参数\n",
    "        \n",
    "        return model"
   ],
   "id": "4d8f27b1d83451f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "CLIP stage 1 的输出无法直接交给FPN，因此需要一个Adapter\n",
    "\n",
    "FPN的实现中有类似作用的定位，即 FPN 的bottom_up，可以用这个来替换\n",
    "\n",
    "使用卷积(kernel_size=1时等同于全连接)+插值来构造多层特征"
   ],
   "id": "f3ff4034b0609afe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CLIPtoFPNAdapter(nn.Module):\n",
    "    def __init__(self, in_channels=768, out_channels_list=[256, 256, 256, 256]):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for out_ch in out_channels_list:\n",
    "            self.convs.append(nn.Conv2d(in_channels, out_ch, kernel_size=1))\n",
    "            in_channels = out_ch\n",
    "            \n",
    "        self.apply(init_weights_xavier)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 768, 7, 7]\n",
    "        c5 = self.convs[0](x)  # [B, 256, 7, 7]\n",
    "        c4 = self.convs[1](F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False))  # [B, 256, 14, 14]\n",
    "        c3 = self.convs[2](F.interpolate(x, scale_factor=4, mode='bilinear', align_corners=False))  # [B, 256, 28, 28]\n",
    "        c2 = self.convs[3](F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=False))  # [B, 256, 56, 56]\n",
    "\n",
    "        features = {\n",
    "            \"res2\": c2,\n",
    "            \"res3\": c3,\n",
    "            \"res4\": c4,\n",
    "            \"res5\": c5,\n",
    "        }\n",
    "        return features"
   ],
   "id": "ac3f7f05c59b7815"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset和Dataloader总体沿用之前的",
   "id": "24fb630a9581b5c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset_name=None, dataset_dicts=None, meta_data=None):\n",
    "        if dataset_name is None:\n",
    "            if dataset_dicts is None or meta_data is None:\n",
    "                raise ValueError(\"dataset_name为None时dataset_dicts与meta_data不能为None\")\n",
    "            else:\n",
    "                self.dataset_dicts = dataset_dicts\n",
    "                self.meta_data = meta_data\n",
    "        else:\n",
    "            self.dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "            self.meta_data = MetadataCatalog.get(dataset_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_dicts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.dataset_dicts[idx].copy()\n",
    "        # 读取 cv2 图像\n",
    "        org_img = cv2.imread(d[\"file_name\"])\n",
    "        # d['cv2'] = org_img  # 不需要了\n",
    "        # 这一部分来自defaults的__call__\n",
    "        # 转 tensor\n",
    "        # img = self.aug.get_transform(org_img).apply_image(org_img)\n",
    "        d['image'] = torch.as_tensor(org_img.astype(\"float32\").transpose(2, 0, 1))\n",
    "        \n",
    "        # 模型还希望在训练时能够有'Instances'，这里也加上\n",
    "        height, width = org_img.shape[:2]\n",
    "        instances = Instances((height, width))\n",
    "        boxes = []\n",
    "        classes = []\n",
    "        for ann in d[\"annotations\"]:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            classes.append(ann[\"category_id\"])\n",
    "        instances.gt_boxes = Boxes(torch.tensor(boxes, dtype=torch.float32))\n",
    "        instances.gt_classes = torch.tensor(classes, dtype=torch.int64)\n",
    "        d[\"instances\"] = instances\n",
    "        \n",
    "        return d\n",
    "\n",
    "def build_batch_loader(dataset_name=None, dataset_dicts=None, meta_data=None, batch_size=1, shuffle=False):\n",
    "    if dataset_name is None:\n",
    "        if dataset_dicts is None or meta_data is None:\n",
    "            raise ValueError(\"dataset_name为None时dataset_dicts与meta_data不能为None\")\n",
    "        else:\n",
    "            dataset = MyDataset(dataset_dicts=dataset_dicts, meta_data=meta_data)\n",
    "    else:\n",
    "        dataset = MyDataset(dataset_name=dataset_name)\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: batch\n",
    "    )"
   ],
   "id": "117c43e9088ebdf7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MyFVLM(nn.Module):\n",
    "    def __init__(self, clip_path, mask_rcnn_cfg_path, mask_rcnn_weight_path, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.clip_model = TwoStageCLIPModel.from_pretrained(clip_path).to(device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(clip_path)\n",
    "        if mask_rcnn_cfg_path is not None:\n",
    "            self.mask_rcnn_model = MyMRCNN2.from_pretrained(mask_rcnn_cfg_path, mask_rcnn_weight_path).to(device)\n",
    "        \n",
    "        self.froze_VLM()\n",
    "        \n",
    "    def froze_VLM(self):\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, batched_inputs, text_embeddings):\n",
    "        if not self.training:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        batch_imgs = [bat[\"image\"] for bat in batched_inputs]\n",
    "        image_sizes = [(bat[\"image\"].shape[1], bat[\"image\"].shape[2]) for bat in batched_inputs]\n",
    "            \n",
    "        clip_feature = self.clip_processor(images=batch_imgs, return_tensors='pt', padding=True).to(self.device)  # TODO: 训练前做好是否可能？\n",
    "        gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n",
    "        \n",
    "        loss_dict = self.mask_rcnn_model(image_sizes, clip_feature, gt_instances, text_embeddings)\n",
    "        \n",
    "        return loss_dict\n",
    "            \n",
    "    def inference(self, batch):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def class_name_list_prepare(self, class_name_list):\n",
    "        #class_name_list = [\"a photo of \" + cls for cls in class_name_list]  # TODO: zero-shot 测试？\n",
    "        class_name_list.append(\"background or no object\")  # TODO: 确认background怎么弄\n",
    "        return class_name_list\n",
    "        \n",
    "    def get_cls_embedding(self, class_name_list):\n",
    "        class_inputs = self.clip_processor(text=class_name_list, return_tensors=\"pt\", padding=True).to(\n",
    "            self.clip_model.device)\n",
    "        return self.clip_model.get_text_features(**class_inputs).to(self.clip_model.device)\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save({\"model\": self.mask_rcnn_model.state_dict()}, path)\n",
    "        \n",
    "    def load(self, path, device):\n",
    "        self.device = device\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.mask_rcnn_model.load_state_dict(checkpoint[\"model\"])"
   ],
   "id": "fe4794df02b691a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "定义训练保存机制",
   "id": "620d7b43d5b1afd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_training_state(model, optimizer, scheduler, epoch, path):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.mask_rcnn_model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict() if scheduler else None\n",
    "    }, path)\n",
    "\n",
    "\n",
    "def load_training_state(model, optimizer, scheduler, path, device=\"cuda\"):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    if checkpoint.get(\"model\") and model.mask_rcnn_model:\n",
    "        model.mask_rcnn_model.load_state_dict(checkpoint[\"model\"])\n",
    "    if checkpoint.get(\"optimizer\") and optimizer:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if checkpoint.get(\"scheduler\") and scheduler:\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "    epoch = checkpoint.get(\"epoch\", 0)\n",
    "    return epoch"
   ],
   "id": "766db088a6a8198f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(model, optimizer, scheduler, epoch_num, batch_size, dataset_name, dataset_path, dataset_json, shuffle=False):\n",
    "    register_coco_instances(dataset_name, {}, dataset_json, dataset_path)\n",
    "    dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "    meta_data = MetadataCatalog.get(dataset_name)\n",
    "    \n",
    "    loader = build_batch_loader(dataset_dicts=dataset_dicts, meta_data=meta_data,\n",
    "                                batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        class_name_list = model.class_name_list_prepare(meta_data.thing_classes)\n",
    "        class_embeddings = model.get_cls_embedding(class_name_list)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        for batch in loader:\n",
    "            losses = model(batch, class_embeddings)\n",
    "            \n",
    "            "
   ],
   "id": "ca67bafc38ca4ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
